{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":46125,"databundleVersionId":4972941,"sourceType":"competition"},{"sourceId":10744281,"sourceType":"datasetVersion","datasetId":6663047}],"dockerImageVersionId":30397,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CLIP With early stopping\n\naccuracy: 0.7321  \nval_accuracy: 0.6949  \nPrecision: 0.4666  \nRecall: 0.4671  \nF1-Score: 0.4604  \nWeighted F1-Score: 0.6787","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    roc_auc = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr')\n    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T16:48:08.022272Z","iopub.execute_input":"2025-02-15T16:48:08.022609Z","iopub.status.idle":"2025-02-15T17:01:10.281724Z","shell.execute_reply.started":"2025-02-15T16:48:08.022539Z","shell.execute_reply":"2025-02-15T17:01:10.280823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping\n\naccuracy: 0.7624  \nval_accuracy: 0.7177     \nPrecision: 0.6341  \nRecall: 0.4891  \nF1-Score: 0.4912  \nWeighted F1-Score: 0.6910    ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:03:53.893384Z","iopub.execute_input":"2025-02-15T17:03:53.893718Z","iopub.status.idle":"2025-02-15T17:19:03.196909Z","shell.execute_reply.started":"2025-02-15T17:03:53.893693Z","shell.execute_reply":"2025-02-15T17:19:03.195916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping  \n## Handle class imbalance + Modify model architecture\n\naccuracy: 0.6405  \nval_accuracy: 0.5863     \nPrecision: 0.5504  \nRecall: 0.6473  \nF1-Score: 0.5263  \nWeighted F1-Score: 0.6227    ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Improved feature fusion\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        \n        # Normalize features\n        image_features = tf.math.l2_normalize(image_features, axis=1)\n        text_features = tf.math.l2_normalize(text_features, axis=1)\n        \n        # Multi-modal interaction\n        combined_features = Concatenate()([\n            image_features, \n            text_features, \n            image_features * text_features  # Cross-modality interaction\n        ])\n        \n        # Enhanced classifier head\n        x = Dense(768, activation='gelu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = BatchNormalization()(x)\n        x = Dense(384, activation='gelu')(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        # Build and compile model\n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        class_weight=class_weights\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n\n    # Handle class imbalance\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels, class_weights\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:19:03.199532Z","iopub.execute_input":"2025-02-15T17:19:03.199812Z","iopub.status.idle":"2025-02-15T17:34:13.978315Z","shell.execute_reply.started":"2025-02-15T17:19:03.199786Z","shell.execute_reply":"2025-02-15T17:34:13.977505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping  \n## Handle class imbalance + Modify model architecture \n## Increased Dropout + Increased learning rate to 2e-4 + epoch 50\n\naccuracy: 0.9311   \nval_accuracy: 0.6960         \nPrecision: 0.5589  \nRecall: 0.5826  \nF1-Score: 0.5679  \nWeighted F1-Score: 0.7027     ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Improved feature fusion\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        \n        # Normalize features\n        image_features = tf.math.l2_normalize(image_features, axis=1)\n        text_features = tf.math.l2_normalize(text_features, axis=1)\n        \n        # Multi-modal interaction\n        combined_features = Concatenate()([\n            image_features, \n            text_features, \n            image_features * text_features  # Cross-modality interaction\n        ])\n        \n        # Enhanced classifier head\n        x = Dense(768, activation='gelu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = BatchNormalization()(x)\n        x = Dense(384, activation='gelu')(x)\n        x = Dropout(0.4)(x) # increased dropout\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        # Build and compile model\n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-4) #increased learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, class_weights, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        class_weight=class_weights\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n\n    # Handle class imbalance\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels, class_weights\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:34:13.980635Z","iopub.execute_input":"2025-02-15T17:34:13.980920Z","iopub.status.idle":"2025-02-15T18:03:52.500812Z","shell.execute_reply.started":"2025-02-15T17:34:13.980889Z","shell.execute_reply":"2025-02-15T18:03:52.500031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping  \n## Handle class imbalance + Modify model architecture \n## Increased Dropout + Decreased learning rate to 2e-5 + epoch 50\n\naccuracy: 0.7218  \nval_accuracy: 0.6400         \nPrecision: 0.5467  \nRecall: 0.6223  \nF1-Score: 0.5521  \nWeighted F1-Score: 0.6651     ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Improved feature fusion\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        \n        # Normalize features\n        image_features = tf.math.l2_normalize(image_features, axis=1)\n        text_features = tf.math.l2_normalize(text_features, axis=1)\n        \n        # Multi-modal interaction\n        combined_features = Concatenate()([\n            image_features, \n            text_features, \n            image_features * text_features  # Cross-modality interaction\n        ])\n        \n        # Enhanced classifier head\n        x = Dense(768, activation='gelu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = BatchNormalization()(x)\n        x = Dense(384, activation='gelu')(x)\n        x = Dropout(0.4)(x) # increased dropout\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        # Build and compile model\n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5) #increased learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, class_weights, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        class_weight=class_weights\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n\n    # Handle class imbalance\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels, class_weights\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:04:35.392518Z","iopub.execute_input":"2025-02-15T18:04:35.392825Z","iopub.status.idle":"2025-02-15T18:34:19.241878Z","shell.execute_reply.started":"2025-02-15T18:04:35.392801Z","shell.execute_reply":"2025-02-15T18:34:19.240937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping  \n## Handle class imbalance + Modify model architecture \n## Unfreeze CLIP layer + epoch 50 (high overfitting)\n\naccuracy: 0.7218  \nval_accuracy: 0.6400         \nPrecision: 0.5467  \nRecall: 0.6223  \nF1-Score: 0.5521  \nWeighted F1-Score: 0.6651     ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Fine-tune the entire CLIP model\n        self.clip.trainable = True\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Improved feature fusion\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        \n        # Normalize features\n        image_features = tf.math.l2_normalize(image_features, axis=1)\n        text_features = tf.math.l2_normalize(text_features, axis=1)\n        \n        # Multi-modal interaction\n        combined_features = Concatenate()([\n            image_features, \n            text_features, \n            image_features * text_features  # Cross-modality interaction\n        ])\n        \n        # Enhanced classifier head\n        x = Dense(768, activation='gelu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = BatchNormalization()(x)\n        x = Dense(384, activation='gelu')(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        # Build and compile model\n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, class_weights, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        class_weight=class_weights\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n\n    # Handle class imbalance\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels, class_weights\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:36:14.934033Z","iopub.execute_input":"2025-02-15T18:36:14.934369Z","iopub.status.idle":"2025-02-15T19:42:59.529071Z","shell.execute_reply.started":"2025-02-15T18:36:14.934342Z","shell.execute_reply":"2025-02-15T19:42:59.528141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping with K-fold Cross validation\n## Handle class imbalance + Modify model architecture \n## Increased Dropout + Decreased learning rate to 2e-5 + epoch 50\n\naccuracy: 0.7218  \nval_accuracy: 0.6400         \nPrecision: 0.5467  \nRecall: 0.6223  \nF1-Score: 0.5521  \nWeighted F1-Score: 0.6651     ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Improved feature fusion\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        \n        # Normalize features\n        image_features = tf.math.l2_normalize(image_features, axis=1)\n        text_features = tf.math.l2_normalize(text_features, axis=1)\n        \n        # Multi-modal interaction\n        combined_features = Concatenate()([\n            image_features, \n            text_features, \n            image_features * text_features  # Cross-modality interaction\n        ])\n        \n        # Enhanced classifier head\n        x = Dense(768, activation='gelu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = BatchNormalization()(x)\n        x = Dense(384, activation='gelu')(x)\n        x = Dropout(0.4)(x) # increased dropout\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        # Build and compile model\n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5) #increased learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        class_weight=class_weights\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\nfrom sklearn.model_selection import StratifiedKFold\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Get image paths and texts\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    image_paths = get_image_paths(memes_folder, combined_df['image_name'].tolist())\n    texts = combined_df['Captions'].tolist()\n    labels = combined_df['Label_Sentiment'].values\n\n    # Initialize k-fold cross-validation\n    k = 4  # Number of folds\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n    fold_results = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels)):\n        print(f\"\\nTraining Fold {fold + 1}/{k}\")\n        \n        # Split data into training and validation sets\n        train_image_paths = [image_paths[i] for i in train_idx]\n        train_texts = [texts[i] for i in train_idx]\n        train_labels = labels[train_idx]\n        \n        val_image_paths = [image_paths[i] for i in val_idx]\n        val_texts = [texts[i] for i in val_idx]\n        val_labels = labels[val_idx]\n\n        # Handle class imbalance\n        class_weights = compute_class_weight(\n            class_weight='balanced',\n            classes=np.unique(train_labels),\n            y=train_labels\n        )\n        class_weights = dict(enumerate(class_weights))\n\n        # Train model\n        model, history = train_model(\n            train_image_paths, train_texts, train_labels,\n            val_image_paths, val_texts, val_labels, class_weights\n        )\n\n        # Evaluate model on the validation set\n        val_inputs = process_data_batch(val_image_paths, val_texts, model_handler=CLIPSentimentModel())\n        evaluate_model(model, val_inputs, val_labels, label_map)\n\n        # Store fold results\n        fold_results.append({\n            'val_accuracy': history.history['val_accuracy'][-1],\n            'val_loss': history.history['val_loss'][-1],\n            'val_f1': f1_score(val_labels, np.argmax(model.predict(val_inputs), axis=1), average='weighted')\n        })\n\n    # Aggregate results across folds\n    avg_val_accuracy = np.mean([result['val_accuracy'] for result in fold_results])\n    avg_val_loss = np.mean([result['val_loss'] for result in fold_results])\n    avg_val_f1 = np.mean([result['val_f1'] for result in fold_results])\n    print(f\"\\nAverage Validation Accuracy: {avg_val_accuracy:.4f}\")\n    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"Average Validation Weighted F1-Score: {avg_val_f1:.4f}\")\n\n    # Train final model on the entire dataset (optional)\n    print(\"\\nTraining final model on the entire dataset...\")\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(labels),\n        y=labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    final_model, _ = train_model(\n        image_paths, texts, labels,\n        image_paths, texts, labels, class_weights  # Use the same data for validation (optional)\n    )\n\n    # Save the final model\n    final_model.save('final_model.h5')\n    print(\"Final model saved to final_model.h5\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T19:20:35.343444Z","iopub.execute_input":"2025-02-14T19:20:35.343783Z","iopub.status.idle":"2025-02-14T20:29:04.129790Z","shell.execute_reply.started":"2025-02-14T19:20:35.343757Z","shell.execute_reply":"2025-02-14T20:29:04.128389Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP best wf score model\n\nModified loss function. Added Focal Loss Instead of Cross-Entropy.  \n\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_addons as tfa\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Improved feature fusion\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        \n        # Normalize features\n        image_features = tf.math.l2_normalize(image_features, axis=1)\n        text_features = tf.math.l2_normalize(text_features, axis=1)\n        \n        # Multi-modal interaction\n        combined_features = Concatenate()([\n            image_features, \n            text_features, \n            image_features * text_features  # Cross-modality interaction\n        ])\n        \n        # Enhanced classifier head\n        x = Dense(768, activation='gelu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = BatchNormalization()(x)\n        x = Dense(384, activation='gelu')(x)\n        x = Dropout(0.4)(x) # increased dropout\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        # Build and compile model\n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-4) #increased learning rate\n\n        # Use focal loss to focus on harder to find examples\n        model.compile(\n            optimizer=Adam(learning_rate=2e-5),\n            loss=tfa.losses.SigmoidFocalCrossEntropy(),\n            metrics=['accuracy']\n        )\n\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, class_weights, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        class_weight=class_weights\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n\n    # Handle class imbalance\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels, class_weights\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:04:48.004447Z","iopub.execute_input":"2025-02-15T20:04:48.004836Z","iopub.status.idle":"2025-02-15T20:04:48.075029Z","shell.execute_reply.started":"2025-02-15T20:04:48.004805Z","shell.execute_reply":"2025-02-15T20:04:48.073717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP best wf score model\n\nAdded more hidden layers to capture complex relationships.\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_addons as tfa\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding='max_length',  # Ensure consistent padding\n            truncation=True,       # Truncate sequences longer than max_length\n            max_length=77          # Default sequence length for CLIP\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Improved feature fusion\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        \n        # Normalize features\n        image_features = tf.math.l2_normalize(image_features, axis=1)\n        text_features = tf.math.l2_normalize(text_features, axis=1)\n        \n        # Multi-modal interaction\n        combined_features = Concatenate()([\n            image_features, \n            text_features, \n            image_features * text_features  # Cross-modality interaction\n        ])\n        \n        # Enhanced classifier head\n        # Added more hidden layers. larger layers help capture complex features\n        x = Dense(768, activation='gelu')(combined_features)  # Increased units\n        x = Dropout(0.4)(x)  # Higher dropout to prevent overfitting\n        x = LayerNormalization()(x)\n        x = Dense(512, activation='gelu')(x)\n        x = Dropout(0.3)(x)\n        x = Dense(256, activation='gelu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        \n        # Build and compile model\n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-4) #increased learning rate\n\n        # Use focal loss to focus on harder to find examples\n        model.compile(\n            optimizer=Adam(learning_rate=2e-5),\n            loss=tfa.losses.SigmoidFocalCrossEntropy(),\n            metrics=['accuracy']\n        )\n\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch with consistent padding\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        \n        # Debug: Print shapes of input_ids and attention_mask\n        # print(f\"Batch {i//batch_size + 1}:\")\n        # print(\"input_ids shape:\", inputs['input_ids'].shape)\n        # print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n        # print(\"pixel_values shape:\", inputs['pixel_values'].shape)\n        \n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, class_weights, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        class_weight=class_weights\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n\ndef evaluate_model(model, test_inputs, test_labels, label_map):\n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision = precision_score(test_labels, predicted_labels, average='macro')\n    recall = recall_score(test_labels, predicted_labels, average='macro')\n    f1 = f1_score(test_labels, predicted_labels, average='macro')\n    weighted_f1 = f1_score(test_labels, predicted_labels, average='weighted')\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, predicted_labels)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # ROC-AUC Curve\n    y_test_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=len(label_map))\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i, label in enumerate(label_map.keys()):\n        fpr[label], tpr[label], _ = roc_curve(y_test_one_hot[:, i], predictions[:, i])\n        roc_auc[label] = auc(fpr[label], tpr[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(fpr[label], tpr[label], label=f'{label} (AUC = {roc_auc[label]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC-AUC Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    # ROC-AUC Score (macro-average)\n    roc_auc_macro = roc_auc_score(y_test_one_hot, predictions, multi_class='ovr', average='macro')\n    print(f\"Macro-average ROC-AUC Score: {roc_auc_macro:.4f}\")\n    \n    # Precision-Recall Curve\n    precision_dict = {}\n    recall_dict = {}\n    average_precision_dict = {}\n    for i, label in enumerate(label_map.keys()):\n        precision_dict[label], recall_dict[label], _ = precision_recall_curve(y_test_one_hot[:, i], predictions[:, i])\n        average_precision_dict[label] = auc(recall_dict[label], precision_dict[label])\n    \n    plt.figure(figsize=(8, 6))\n    for label in label_map.keys():\n        plt.plot(recall_dict[label], precision_dict[label], label=f'{label} (AP = {average_precision_dict[label]:.2f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef main():\n    # Load the combined dataset with the correct encoding\n    try:\n        combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='utf-8')\n    except UnicodeDecodeError:\n        # Try alternative encodings if UTF-8 fails\n        try:\n            combined_df = pd.read_csv('/kaggle/input/memosen-with-label/multi-sent.csv', encoding='latin-1')\n        except Exception as e:\n            print(f\"Failed to read the CSV file: {e}\")\n            return\n    \n    # Debug: Print column names to verify\n    print(\"Combined DataFrame Columns:\", combined_df.columns)\n    \n    # Ensure the label column exists\n    if 'Label_Sentiment' not in combined_df.columns:\n        raise KeyError(\"Column 'Label_Sentiment' not found in the combined dataset. Please check the column names.\")\n    \n    # Check for missing or invalid values in the 'Label_Sentiment' column\n    print(\"Number of missing values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].isna().sum())\n    print(\"Unique values in 'Label_Sentiment':\", combined_df['Label_Sentiment'].unique())\n    \n    # Drop rows with missing or invalid labels\n    valid_labels = ['positive', 'neutral', 'negative']\n    combined_df = combined_df[combined_df['Label_Sentiment'].isin(valid_labels)]\n    \n    # Check if the dataset is empty after cleaning\n    if combined_df.empty:\n        raise ValueError(\"The dataset is empty after removing rows with invalid labels.\")\n    \n    # Convert labels to numerical values\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    combined_df['Label_Sentiment'] = combined_df['Label_Sentiment'].map(label_map)\n    \n    # Split the data into train and test sets (80-20 ratio)\n    train_df, test_df = train_test_split(\n        combined_df, test_size=0.2, random_state=42, stratify=combined_df['Label_Sentiment']\n    )\n    \n    # Copy the labels of the test set for evaluation\n    test_labels_df = test_df[['Label_Sentiment']].copy()\n    \n    # Drop the labels from the test set to simulate unseen data\n    test_df = test_df.drop(columns=['Label_Sentiment'])\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Extract labels\n    train_labels = train_df['Label_Sentiment'].values\n    test_labels = test_labels_df['Label_Sentiment'].values\n\n    # Handle class imbalance\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n    \n    # Train model\n    model, history = train_model(\n        train_image_paths, train_df['Captions'].tolist(), train_labels,\n        test_image_paths, test_df['Captions'].tolist(), test_labels, class_weights\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Evaluate model on the test set\n    evaluate_model(model, test_inputs, test_labels, label_map)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to original labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Check if 'Id' column exists in test_df\n    if 'Id' not in test_df.columns:\n        # Create a unique identifier if 'Id' column is missing\n        test_df['Id'] = range(1, len(test_df) + 1)\n        print(\"Warning: 'Id' column not found in test_df. A unique identifier has been created.\")\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}