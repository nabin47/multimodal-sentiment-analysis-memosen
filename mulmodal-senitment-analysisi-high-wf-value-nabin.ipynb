{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":46125,"databundleVersionId":4972941,"sourceType":"competition"}],"dockerImageVersionId":30397,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center;font-weight: 900; font-size:40px;\"> Multimodal Sentiment Analysis Higher Accuracy </p>","metadata":{}},{"cell_type":"markdown","source":"**More robust vgg19 and xlm-roberta******","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport os\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=3,\n            min_lr=1e-6\n        )\n    ]\n\n    # Data augmentation for images\n    train_images_augmented = augment_images(train_images)\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:24:30.074829Z","iopub.execute_input":"2025-01-06T18:24:30.075204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nimport os\nimport math\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef cosine_decay(epoch):\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(epoch * math.pi / 20)) / 2\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=2,  # Stop earlier\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(cosine_decay)\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:11:00.158581Z","iopub.execute_input":"2025-01-06T18:11:00.158941Z","iopub.status.idle":"2025-01-06T18:22:16.469812Z","shell.execute_reply.started":"2025-01-06T18:11:00.158912Z","shell.execute_reply":"2025-01-06T18:22:16.46908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This vgg19 and bert model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:01:08.238623Z","iopub.execute_input":"2025-01-06T17:01:08.239137Z","execution_failed":"2025-01-06T17:34:25.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:28:02.213845Z","iopub.execute_input":"2025-01-06T15:28:02.214167Z","iopub.status.idle":"2025-01-06T16:38:51.269392Z","shell.execute_reply.started":"2025-01-06T15:28:02.214142Z","shell.execute_reply":"2025-01-06T16:38:51.268536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=10):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:11:27.045036Z","iopub.execute_input":"2025-01-06T15:11:27.045356Z","iopub.status.idle":"2025-01-06T15:27:46.780771Z","shell.execute_reply.started":"2025-01-06T15:11:27.045331Z","shell.execute_reply":"2025-01-06T15:27:46.779955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T10:43:23.668372Z","iopub.execute_input":"2025-01-06T10:43:23.6687Z","iopub.status.idle":"2025-01-06T11:36:22.03778Z","shell.execute_reply.started":"2025-01-06T10:43:23.668674Z","shell.execute_reply":"2025-01-06T11:36:22.03688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Less layer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T11:41:58.166279Z","iopub.execute_input":"2025-01-06T11:41:58.166641Z","iopub.status.idle":"2025-01-06T12:34:48.887549Z","shell.execute_reply.started":"2025-01-06T11:41:58.166615Z","shell.execute_reply":"2025-01-06T12:34:48.886596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:13:06.320848Z","iopub.execute_input":"2025-01-06T14:13:06.321144Z","iopub.status.idle":"2025-01-06T15:03:11.81315Z","shell.execute_reply.started":"2025-01-06T14:13:06.321084Z","shell.execute_reply":"2025-01-06T15:03:11.812201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Edited","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\n\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs,\n        class_weights_dict\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Evaluate model\n    print(classification_report(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n    print(confusion_matrix(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n\n    # Save predictions\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:41:14.208845Z","iopub.execute_input":"2025-01-06T16:41:14.209344Z","iopub.status.idle":"2025-01-06T16:52:55.961419Z","shell.execute_reply.started":"2025-01-06T16:41:14.209314Z","shell.execute_reply":"2025-01-06T16:52:55.960223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\nimport tensorflow_addons as tfa\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Focal loss for better handling of class imbalance\n    loss = tfa.losses.SigmoidFocalCrossEntropy()\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss=loss,\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:38:09.070207Z","iopub.execute_input":"2025-01-06T18:38:09.070634Z","iopub.status.idle":"2025-01-06T18:41:26.481732Z","shell.execute_reply.started":"2025-01-06T18:38:09.070548Z","shell.execute_reply":"2025-01-06T18:41:26.480188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Convert labels to one-hot encoding\n    train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n    val_labels_onehot = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels_onehot,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels_onehot\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:47:32.681831Z","iopub.execute_input":"2025-01-06T18:47:32.682182Z","iopub.status.idle":"2025-01-06T18:59:22.905496Z","shell.execute_reply.started":"2025-01-06T18:47:32.682107Z","shell.execute_reply":"2025-01-06T18:59:22.904502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T05:53:13.106974Z","iopub.execute_input":"2025-01-07T05:53:13.107721Z","iopub.status.idle":"2025-01-07T05:53:32.898832Z","shell.execute_reply.started":"2025-01-07T05:53:13.107689Z","shell.execute_reply":"2025-01-07T05:53:32.8977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu')(x)  # Add Dense layer instead of pooling\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T06:13:41.793368Z","iopub.execute_input":"2025-01-07T06:13:41.79381Z","iopub.status.idle":"2025-01-07T06:45:12.973797Z","shell.execute_reply.started":"2025-01-07T06:13:41.793732Z","shell.execute_reply":"2025-01-07T06:45:12.972784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:33:48.16119Z","iopub.execute_input":"2025-01-07T08:33:48.161544Z","iopub.status.idle":"2025-01-07T08:34:09.481692Z","shell.execute_reply.started":"2025-01-07T08:33:48.161516Z","shell.execute_reply":"2025-01-07T08:34:09.480417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',  # Monitor validation loss\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,  # Decreased batch size for better generalization\n        class_weight=class_weight_dict,  # Add class weights\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:34:31.838915Z","iopub.execute_input":"2025-01-07T08:34:31.839388Z","execution_failed":"2025-01-07T09:14:48.518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow pandas numpy tqdm scikit-learn transformers vit-keras\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:17:53.255584Z","iopub.execute_input":"2025-01-07T09:17:53.255924Z","iopub.status.idle":"2025-01-07T09:18:13.372949Z","shell.execute_reply.started":"2025-01-07T09:17:53.2559Z","shell.execute_reply":"2025-01-07T09:18:13.371612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import LabelEncoder\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n# Image augmentation\ndef augment_images(images):\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=len(images), shuffle=False).next()\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Augment training images\n    train_images = augment_images(train_images)\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        class_weight=class_weight_dict,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:18:28.830186Z","iopub.execute_input":"2025-01-07T09:18:28.830493Z","execution_failed":"2025-01-07T09:36:14.932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VGG19 + BERT with Regularizaition and 0.4 dropout \nAdded regularization and high dropout rate to avoid overfitting.\nBatch size: 32\nLayer: 256","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\n\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs,\n        class_weights_dict\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Evaluate model\n    print(classification_report(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n    print(confusion_matrix(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n\n    # Save predictions\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VisualBERT multimodal model default settings","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import TFVisualBertModel, VisualBertConfig, BertTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport torch\nfrom torchvision.models import resnet50\nfrom torchvision.transforms import Compose, Resize, ToTensor, Normalize\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, transform):\n    \"\"\"Load and preprocess a single image using PyTorch transforms\"\"\"\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return torch.zeros((3, 224, 224))\n\ndef process_images(image_paths):\n    \"\"\"Process all images and extract features using ResNet\"\"\"\n    transform = Compose([\n        Resize((224, 224)),\n        ToTensor(),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load ResNet model\n    resnet = resnet50(pretrained=True)\n    resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # Remove classification layer\n    resnet.eval()\n    \n    features = []\n    with torch.no_grad():\n        for path in tqdm(image_paths, desc=\"Processing images\"):\n            img = preprocess_image(path, transform)\n            img = img.unsqueeze(0)  # Add batch dimension\n            feat = resnet(img)\n            features.append(feat.squeeze().numpy())\n    \n    return np.array(features)\n\nclass VisualBertSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.config = VisualBertConfig.from_pretrained('uclanlp/visualbert-vqa')\n        self.visual_bert = TFVisualBertModel.from_pretrained('uclanlp/visualbert-vqa', config=self.config)\n        \n    def build_model(self):\n        # Inputs\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        visual_features = Input(shape=(2048,), dtype=tf.float32, name='visual_features')\n        visual_attention_mask = Input(shape=(1,), dtype=tf.int32, name='visual_attention_mask')\n        \n        # Expand visual features dimensions to match VisualBERT requirements\n        visual_embeds = tf.expand_dims(visual_features, axis=1)  # Replace unsqueeze with expand_dims\n        \n        # Pass through VisualBERT\n        outputs = self.visual_bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            visual_embeddings=visual_embeds,\n            visual_attention_mask=visual_attention_mask\n        )\n        \n        # Get pooled output\n        pooled_output = outputs[1]\n        \n        # Classification layers\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, visual_features, visual_attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = VisualBertSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Prepare visual attention masks\n    train_visual_attention_mask = tf.ones((train_images.shape[0], 1), dtype=tf.int32)\n    val_visual_attention_mask = tf.ones((val_images.shape[0], 1), dtype=tf.int32)\n    \n    # Convert images to float32\n    train_images = tf.cast(train_images, tf.float32)\n    val_images = tf.cast(val_images, tf.float32)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask,\n            'visual_features': train_images,\n            'visual_attention_mask': train_visual_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask,\n                'visual_features': val_images,\n                'visual_attention_mask': val_visual_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths and extract features\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    train_features = process_images(train_image_paths)\n    test_features = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_feats, val_feats, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_features, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_feats, train_texts, train_labs,\n        val_feats, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = VisualBertSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    test_visual_attention_mask = tf.ones((test_features.shape[0], 1), dtype=tf.int32)\n    test_features = tf.cast(test_features, tf.float32)\n    \n    # Make predictions\n    predictions = model.predict({\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask,\n        'visual_features': test_features,\n        'visual_attention_mask': test_visual_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:18:54.091720Z","iopub.execute_input":"2025-01-09T07:18:54.092139Z","iopub.status.idle":"2025-01-09T07:18:54.141985Z","shell.execute_reply.started":"2025-01-09T07:18:54.092108Z","shell.execute_reply":"2025-01-09T07:18:54.140727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP model with early stopping\n\nStopped at: epoch 14\nEpoch: 30\\\nAccuracy: 0.7384\\\nVal. accuracy: 0.7295\\\nLayers: 512->256\\\nBatch size: 32","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:20:30.294262Z","iopub.execute_input":"2025-01-09T07:20:30.294642Z","iopub.status.idle":"2025-01-09T07:31:17.890927Z","shell.execute_reply.started":"2025-01-09T07:20:30.294610Z","shell.execute_reply":"2025-01-09T07:31:17.889953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping\nEpoch: 30\\\nAccuracy: 0.7929\\\nVal. accuracy: 0.7371\\\nLayers: 512->256\\\nBatch size: 32","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T08:05:39.434689Z","iopub.execute_input":"2025-01-09T08:05:39.435060Z","iopub.status.idle":"2025-01-09T08:22:16.467403Z","shell.execute_reply.started":"2025-01-09T08:05:39.435031Z","shell.execute_reply":"2025-01-09T08:22:16.466453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping v-2.0\nEpoch: 20\\\nAccuracy: 0.7535\\\nVal. accuracy: 0.7295\\\nLayers: 256->128\\\nBatch size: 16","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(256, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T08:47:41.017400Z","iopub.execute_input":"2025-01-09T08:47:41.017792Z","iopub.status.idle":"2025-01-09T09:00:45.160275Z","shell.execute_reply.started":"2025-01-09T08:47:41.017713Z","shell.execute_reply":"2025-01-09T09:00:45.159287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-3\n## Increased dropout, decreased batch size\nEpoch: 20\\\nAccuracy: 0.7576\\\nVal. accuracy: 0.7371\\\nLayers: 512->256\\\nBatch size: 16\\\nDropout: 0.4","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T13:18:51.713664Z","iopub.execute_input":"2025-01-09T13:18:51.714058Z","iopub.status.idle":"2025-01-09T13:32:42.382109Z","shell.execute_reply.started":"2025-01-09T13:18:51.713972Z","shell.execute_reply":"2025-01-09T13:32:42.381137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-4\n## Added kernel regularizer\nEpoch: 20\\\nAccuracy: 0.7613\\\nVal. accuracy: 0.7390\\\nLayers: 512->256\\\nBatch size: 16\\\nDropout: 0.4","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T13:47:49.471280Z","iopub.execute_input":"2025-01-09T13:47:49.471670Z","iopub.status.idle":"2025-01-09T14:00:21.073949Z","shell.execute_reply.started":"2025-01-09T13:47:49.471624Z","shell.execute_reply":"2025-01-09T14:00:21.073027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-5\n## Increase batch size and decay\nEpoch: 30\\\nAccuracy: 0.7613\\\nVal. accuracy: 0.7390\\\nLayers: 512->256\\\nBatch size: 32\\\nDropout: 0.4\\\nRegularization decay: 0.2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:31:30.678232Z","iopub.execute_input":"2025-01-09T14:31:30.678548Z","iopub.status.idle":"2025-01-09T14:54:29.868943Z","shell.execute_reply.started":"2025-01-09T14:31:30.678522Z","shell.execute_reply":"2025-01-09T14:54:29.867954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-6\n## Same as v-5 but increased dropout in two layers instead of just one\nEpoch: 50\\\nAccuracy: 0.7818\\\nVal. accuracy: 0.7486\\\nLayers: 512->256\\\nBatch size: 32\\\nDropout: 0.4\\\nRegularization decay: 0.2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(x)\n        x = Dropout(0.4)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:02:28.814112Z","iopub.execute_input":"2025-01-09T15:02:28.814733Z","iopub.status.idle":"2025-01-09T15:25:28.566506Z","shell.execute_reply.started":"2025-01-09T15:02:28.814671Z","shell.execute_reply":"2025-01-09T15:25:28.565374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-7\n## Same as v-6 but used 30 epochs\nEpoch: 30\\\nAccuracy: 0.7613\\\nVal. accuracy: 0.7390\\\nLayers: 512->256\\\nBatch size: 32\\\nDropout: 0.4\\\nRegularization decay: 0.2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(x)\n        x = Dropout(0.4)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:41:19.020378Z","iopub.execute_input":"2025-01-09T15:41:19.020720Z","iopub.status.idle":"2025-01-09T15:56:54.509250Z","shell.execute_reply.started":"2025-01-09T15:41:19.020694Z","shell.execute_reply":"2025-01-09T15:56:54.508268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FLAVA model v-1\nDefault FLAVA implemenation on the best result of CLIP\n\nEpoch: 30\\\nAccuracy: 74.92%\\\nVal. accuracy: 74.10%\\\nLayers: 512->512->512->256\\\nBatch size: 16\\\nDropout: 0.4\\\nWeight decay: 0.2","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T05:41:19.565927Z","iopub.execute_input":"2025-01-10T05:41:19.566297Z","iopub.status.idle":"2025-01-10T05:41:29.926771Z","shell.execute_reply.started":"2025-01-10T05:41:19.566266Z","shell.execute_reply":"2025-01-10T05:41:29.925507Z"}},"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.12.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import FlavaProcessor, FlavaModel\nfrom torch import nn\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nclass FLAVASentimentModel(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        self.num_classes = num_classes\n        self.processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n        self.flava = FlavaModel.from_pretrained(\"facebook/flava-full\")\n        \n        for param in self.flava.parameters():\n            param.requires_grad = False\n            \n        # Get multimodal output size from FLAVA config\n        hidden_size = self.flava.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, image_inputs, text_inputs):\n        outputs = self.flava(\n            input_ids=text_inputs['input_ids'],\n            attention_mask=text_inputs['attention_mask'],\n            pixel_values=image_inputs['pixel_values'],\n            return_dict=True\n        )\n        \n        # Get first token ([CLS]) of multimodal output\n        multimodal_output = outputs.multimodal_embeddings[:, 0]\n        return self.classifier(multimodal_output)\n\ndef process_data_batch(image_paths, texts, processor, batch_size=16, device='cuda'):\n    all_image_inputs = []\n    all_text_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        inputs = processor(\n            images=batch_images,\n            text=batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=77\n        )\n        \n        all_image_inputs.append({\n            'pixel_values': inputs['pixel_values'].to(device)\n        })\n        all_text_inputs.append({\n            'input_ids': inputs['input_ids'].to(device),\n            'attention_mask': inputs['attention_mask'].to(device)\n        })\n    \n    return all_image_inputs, all_text_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, \n                device='cuda', epochs=30, batch_size=16):\n    \n    model = FLAVASentimentModel().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.02)\n    criterion = nn.CrossEntropyLoss()\n    \n    print(\"Processing training data...\")\n    train_image_inputs, train_text_inputs = process_data_batch(\n        train_image_paths, train_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    print(\"Processing validation data...\")\n    val_image_inputs, val_text_inputs = process_data_batch(\n        val_image_paths, val_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    train_labels = torch.tensor(train_labels, device=device)\n    val_labels = torch.tensor(val_labels, device=device)\n    \n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for i in range(len(train_image_inputs)):\n            optimizer.zero_grad()\n            outputs = model(train_image_inputs[i], train_text_inputs[i])\n            batch_labels = train_labels[i * batch_size:(i + 1) * batch_size]\n            \n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += batch_labels.size(0)\n            correct += predicted.eq(batch_labels).sum().item()\n        \n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for i in range(len(val_image_inputs)):\n                outputs = model(val_image_inputs[i], val_text_inputs[i])\n                batch_labels = val_labels[i * batch_size:(i + 1) * batch_size]\n                \n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n        \n        val_acc = 100. * val_correct / val_total\n        print(f'Epoch {epoch + 1}: Train Acc: {100.*correct/total:.2f}% Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return model\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    batch_size = 16\n    \n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    model = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs,\n        device=device,\n        batch_size=batch_size\n    )\n    \n    print(\"Processing test data...\")\n    test_image_inputs, test_text_inputs = process_data_batch(\n        test_image_paths, test_df['Captions'].tolist(),\n        model.processor, batch_size=batch_size, device=device\n    )\n    \n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for i in range(len(test_image_inputs)):\n            outputs = model(test_image_inputs[i], test_text_inputs[i])\n            _, predicted = outputs.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predictions]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T04:35:15.371422Z","iopub.execute_input":"2025-01-10T04:35:15.371785Z","iopub.status.idle":"2025-01-10T05:28:37.364703Z","shell.execute_reply.started":"2025-01-10T04:35:15.371756Z","shell.execute_reply":"2025-01-10T05:28:37.363700Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'mmm_image_head.decoder.bias', 'itm_head.seq_relationship.bias', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'mmm_image_head.bias', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'itm_head.seq_relationship.weight', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'mim_head.transform.dense.bias', 'mmm_image_head.transform.dense.bias', 'mmm_image_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.input.bias', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'mmm_text_head.decoder.weight', 'mim_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'mim_head.decoder.weight', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'mim_head.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'mmm_text_head.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'mmm_text_head.transform.LayerNorm.bias', 'mmm_text_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'mlm_head.transform.dense.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'mim_head.transform.LayerNorm.weight', 'mlm_head.transform.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'mmm_text_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias']\n- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\nProcessing validation data...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:813: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Acc: 61.72% Val Acc: 61.90%\nEpoch 2: Train Acc: 63.67% Val Acc: 62.86%\nEpoch 3: Train Acc: 63.23% Val Acc: 65.33%\nEpoch 4: Train Acc: 64.78% Val Acc: 65.14%\nEpoch 5: Train Acc: 64.21% Val Acc: 66.10%\nEpoch 6: Train Acc: 64.51% Val Acc: 67.24%\nEpoch 7: Train Acc: 66.36% Val Acc: 68.95%\nEpoch 8: Train Acc: 65.69% Val Acc: 67.62%\nEpoch 9: Train Acc: 66.09% Val Acc: 68.57%\nEpoch 10: Train Acc: 67.64% Val Acc: 69.33%\nEpoch 11: Train Acc: 67.37% Val Acc: 69.52%\nEpoch 12: Train Acc: 67.54% Val Acc: 70.29%\nEpoch 13: Train Acc: 68.75% Val Acc: 71.05%\nEpoch 14: Train Acc: 68.86% Val Acc: 71.43%\nEpoch 15: Train Acc: 69.16% Val Acc: 72.19%\nEpoch 16: Train Acc: 71.08% Val Acc: 71.62%\nEpoch 17: Train Acc: 70.71% Val Acc: 72.57%\nEpoch 18: Train Acc: 71.18% Val Acc: 72.38%\nEpoch 19: Train Acc: 71.35% Val Acc: 72.38%\nEpoch 20: Train Acc: 72.32% Val Acc: 73.71%\nEpoch 21: Train Acc: 72.22% Val Acc: 73.33%\nEpoch 22: Train Acc: 72.53% Val Acc: 73.71%\nEpoch 23: Train Acc: 73.27% Val Acc: 74.67%\nEpoch 24: Train Acc: 73.40% Val Acc: 75.05%\nEpoch 25: Train Acc: 73.00% Val Acc: 74.10%\nEpoch 26: Train Acc: 74.48% Val Acc: 74.67%\nEpoch 27: Train Acc: 73.74% Val Acc: 74.67%\nEpoch 28: Train Acc: 73.94% Val Acc: 73.90%\nEpoch 29: Train Acc: 75.29% Val Acc: 74.29%\nEpoch 30: Train Acc: 74.92% Val Acc: 74.10%\nProcessing test data...\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":3}]}