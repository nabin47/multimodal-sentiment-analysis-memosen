{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":46125,"databundleVersionId":4972941,"sourceType":"competition"}],"dockerImageVersionId":30397,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center;font-weight: 900; font-size:40px;\"> Multimodal Sentiment Analysis Higher Accuracy </p>","metadata":{}},{"cell_type":"markdown","source":"**More robust vgg19 and xlm-roberta******","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport os\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=3,\n            min_lr=1e-6\n        )\n    ]\n\n    # Data augmentation for images\n    train_images_augmented = augment_images(train_images)\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:24:30.074829Z","iopub.execute_input":"2025-01-06T18:24:30.075204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nimport os\nimport math\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef cosine_decay(epoch):\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(epoch * math.pi / 20)) / 2\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=2,  # Stop earlier\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(cosine_decay)\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:11:00.158581Z","iopub.execute_input":"2025-01-06T18:11:00.158941Z","iopub.status.idle":"2025-01-06T18:22:16.469812Z","shell.execute_reply.started":"2025-01-06T18:11:00.158912Z","shell.execute_reply":"2025-01-06T18:22:16.46908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This vgg19 and bert model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:01:08.238623Z","iopub.execute_input":"2025-01-06T17:01:08.239137Z","execution_failed":"2025-01-06T17:34:25.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:28:02.213845Z","iopub.execute_input":"2025-01-06T15:28:02.214167Z","iopub.status.idle":"2025-01-06T16:38:51.269392Z","shell.execute_reply.started":"2025-01-06T15:28:02.214142Z","shell.execute_reply":"2025-01-06T16:38:51.268536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=10):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:11:27.045036Z","iopub.execute_input":"2025-01-06T15:11:27.045356Z","iopub.status.idle":"2025-01-06T15:27:46.780771Z","shell.execute_reply.started":"2025-01-06T15:11:27.045331Z","shell.execute_reply":"2025-01-06T15:27:46.779955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T10:43:23.668372Z","iopub.execute_input":"2025-01-06T10:43:23.6687Z","iopub.status.idle":"2025-01-06T11:36:22.03778Z","shell.execute_reply.started":"2025-01-06T10:43:23.668674Z","shell.execute_reply":"2025-01-06T11:36:22.03688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Less layer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T11:41:58.166279Z","iopub.execute_input":"2025-01-06T11:41:58.166641Z","iopub.status.idle":"2025-01-06T12:34:48.887549Z","shell.execute_reply.started":"2025-01-06T11:41:58.166615Z","shell.execute_reply":"2025-01-06T12:34:48.886596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:13:06.320848Z","iopub.execute_input":"2025-01-06T14:13:06.321144Z","iopub.status.idle":"2025-01-06T15:03:11.81315Z","shell.execute_reply.started":"2025-01-06T14:13:06.321084Z","shell.execute_reply":"2025-01-06T15:03:11.812201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Edited","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\n\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs,\n        class_weights_dict\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Evaluate model\n    print(classification_report(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n    print(confusion_matrix(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n\n    # Save predictions\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:41:14.208845Z","iopub.execute_input":"2025-01-06T16:41:14.209344Z","iopub.status.idle":"2025-01-06T16:52:55.961419Z","shell.execute_reply.started":"2025-01-06T16:41:14.209314Z","shell.execute_reply":"2025-01-06T16:52:55.960223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\nimport tensorflow_addons as tfa\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Focal loss for better handling of class imbalance\n    loss = tfa.losses.SigmoidFocalCrossEntropy()\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss=loss,\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:38:09.070207Z","iopub.execute_input":"2025-01-06T18:38:09.070634Z","iopub.status.idle":"2025-01-06T18:41:26.481732Z","shell.execute_reply.started":"2025-01-06T18:38:09.070548Z","shell.execute_reply":"2025-01-06T18:41:26.480188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Convert labels to one-hot encoding\n    train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n    val_labels_onehot = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels_onehot,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels_onehot\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:47:32.681831Z","iopub.execute_input":"2025-01-06T18:47:32.682182Z","iopub.status.idle":"2025-01-06T18:59:22.905496Z","shell.execute_reply.started":"2025-01-06T18:47:32.682107Z","shell.execute_reply":"2025-01-06T18:59:22.904502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T05:53:13.106974Z","iopub.execute_input":"2025-01-07T05:53:13.107721Z","iopub.status.idle":"2025-01-07T05:53:32.898832Z","shell.execute_reply.started":"2025-01-07T05:53:13.107689Z","shell.execute_reply":"2025-01-07T05:53:32.8977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu')(x)  # Add Dense layer instead of pooling\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T06:13:41.793368Z","iopub.execute_input":"2025-01-07T06:13:41.79381Z","iopub.status.idle":"2025-01-07T06:45:12.973797Z","shell.execute_reply.started":"2025-01-07T06:13:41.793732Z","shell.execute_reply":"2025-01-07T06:45:12.972784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:33:48.16119Z","iopub.execute_input":"2025-01-07T08:33:48.161544Z","iopub.status.idle":"2025-01-07T08:34:09.481692Z","shell.execute_reply.started":"2025-01-07T08:33:48.161516Z","shell.execute_reply":"2025-01-07T08:34:09.480417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',  # Monitor validation loss\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,  # Decreased batch size for better generalization\n        class_weight=class_weight_dict,  # Add class weights\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:34:31.838915Z","iopub.execute_input":"2025-01-07T08:34:31.839388Z","execution_failed":"2025-01-07T09:14:48.518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow pandas numpy tqdm scikit-learn transformers vit-keras\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:17:53.255584Z","iopub.execute_input":"2025-01-07T09:17:53.255924Z","iopub.status.idle":"2025-01-07T09:18:13.372949Z","shell.execute_reply.started":"2025-01-07T09:17:53.2559Z","shell.execute_reply":"2025-01-07T09:18:13.371612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import LabelEncoder\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n# Image augmentation\ndef augment_images(images):\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=len(images), shuffle=False).next()\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Augment training images\n    train_images = augment_images(train_images)\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        class_weight=class_weight_dict,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:18:28.830186Z","iopub.execute_input":"2025-01-07T09:18:28.830493Z","execution_failed":"2025-01-07T09:36:14.932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VGG19 + BERT with Regularizaition and 0.4 dropout \nAdded regularization and high dropout rate to avoid overfitting.\nBatch size: 32\nLayer: 256","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\n\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs,\n        class_weights_dict\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Evaluate model\n    print(classification_report(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n    print(confusion_matrix(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n\n    # Save predictions\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VisualBERT multimodal model default settings","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import TFVisualBertModel, VisualBertConfig, BertTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport torch\nfrom torchvision.models import resnet50\nfrom torchvision.transforms import Compose, Resize, ToTensor, Normalize\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, transform):\n    \"\"\"Load and preprocess a single image using PyTorch transforms\"\"\"\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return torch.zeros((3, 224, 224))\n\ndef process_images(image_paths):\n    \"\"\"Process all images and extract features using ResNet\"\"\"\n    transform = Compose([\n        Resize((224, 224)),\n        ToTensor(),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load ResNet model\n    resnet = resnet50(pretrained=True)\n    resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # Remove classification layer\n    resnet.eval()\n    \n    features = []\n    with torch.no_grad():\n        for path in tqdm(image_paths, desc=\"Processing images\"):\n            img = preprocess_image(path, transform)\n            img = img.unsqueeze(0)  # Add batch dimension\n            feat = resnet(img)\n            features.append(feat.squeeze().numpy())\n    \n    return np.array(features)\n\nclass VisualBertSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.config = VisualBertConfig.from_pretrained('uclanlp/visualbert-vqa')\n        self.visual_bert = TFVisualBertModel.from_pretrained('uclanlp/visualbert-vqa', config=self.config)\n        \n    def build_model(self):\n        # Inputs\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        visual_features = Input(shape=(2048,), dtype=tf.float32, name='visual_features')\n        visual_attention_mask = Input(shape=(1,), dtype=tf.int32, name='visual_attention_mask')\n        \n        # Expand visual features dimensions to match VisualBERT requirements\n        visual_embeds = tf.expand_dims(visual_features, axis=1)  # Replace unsqueeze with expand_dims\n        \n        # Pass through VisualBERT\n        outputs = self.visual_bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            visual_embeddings=visual_embeds,\n            visual_attention_mask=visual_attention_mask\n        )\n        \n        # Get pooled output\n        pooled_output = outputs[1]\n        \n        # Classification layers\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, visual_features, visual_attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = VisualBertSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Prepare visual attention masks\n    train_visual_attention_mask = tf.ones((train_images.shape[0], 1), dtype=tf.int32)\n    val_visual_attention_mask = tf.ones((val_images.shape[0], 1), dtype=tf.int32)\n    \n    # Convert images to float32\n    train_images = tf.cast(train_images, tf.float32)\n    val_images = tf.cast(val_images, tf.float32)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask,\n            'visual_features': train_images,\n            'visual_attention_mask': train_visual_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask,\n                'visual_features': val_images,\n                'visual_attention_mask': val_visual_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths and extract features\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    train_features = process_images(train_image_paths)\n    test_features = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_feats, val_feats, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_features, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_feats, train_texts, train_labs,\n        val_feats, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = VisualBertSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    test_visual_attention_mask = tf.ones((test_features.shape[0], 1), dtype=tf.int32)\n    test_features = tf.cast(test_features, tf.float32)\n    \n    # Make predictions\n    predictions = model.predict({\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask,\n        'visual_features': test_features,\n        'visual_attention_mask': test_visual_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:18:54.091720Z","iopub.execute_input":"2025-01-09T07:18:54.092139Z","iopub.status.idle":"2025-01-09T07:18:54.141985Z","shell.execute_reply.started":"2025-01-09T07:18:54.092108Z","shell.execute_reply":"2025-01-09T07:18:54.140727Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2426958270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFVisualBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVisualBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'TFVisualBertModel' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'TFVisualBertModel' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"# CLIP model with early stopping\n\nStopped at: epoch 14\nEpoch: 30\\\nAccuracy: 0.7384\\\nVal. accuracy: 0.7295\\\nLayers: 512->256\\\nBatch size: 32","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:20:30.294262Z","iopub.execute_input":"2025-01-09T07:20:30.294642Z","iopub.status.idle":"2025-01-09T07:31:17.890927Z","shell.execute_reply.started":"2025-01-09T07:20:30.294610Z","shell.execute_reply":"2025-01-09T07:31:17.889953Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef2132a41284b3ea4a3e53707bf2348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fbdc47e89894e4489cf24b42c453b03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/842k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f898ea9752f4f6fa255642cf834e3c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/512k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"669a0c3b59ed4e7ea1b59d6c11379ffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a6e6f1172a4dac983410c7a6d20c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c27a76b2284d4bab04c4684083038a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee0e8046bb04f84a19c75c927c25716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/577M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9725045e46d480cb44052bff644e621"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\nProcessing validation data...\nEpoch 1/30\n93/93 [==============================] - 54s 358ms/step - loss: 0.9300 - accuracy: 0.5751 - val_loss: 0.7694 - val_accuracy: 0.6743 - lr: 2.0000e-05\nEpoch 2/30\n93/93 [==============================] - 23s 252ms/step - loss: 0.8500 - accuracy: 0.6236 - val_loss: 0.7374 - val_accuracy: 0.7048 - lr: 2.0000e-05\nEpoch 3/30\n93/93 [==============================] - 24s 261ms/step - loss: 0.7989 - accuracy: 0.6492 - val_loss: 0.7122 - val_accuracy: 0.7105 - lr: 2.0000e-05\nEpoch 4/30\n93/93 [==============================] - 25s 267ms/step - loss: 0.7776 - accuracy: 0.6572 - val_loss: 0.7101 - val_accuracy: 0.7124 - lr: 2.0000e-05\nEpoch 5/30\n93/93 [==============================] - 24s 261ms/step - loss: 0.7610 - accuracy: 0.6801 - val_loss: 0.6826 - val_accuracy: 0.7162 - lr: 2.0000e-05\nEpoch 6/30\n93/93 [==============================] - 24s 259ms/step - loss: 0.7341 - accuracy: 0.6916 - val_loss: 0.6811 - val_accuracy: 0.7181 - lr: 2.0000e-05\nEpoch 7/30\n93/93 [==============================] - 25s 264ms/step - loss: 0.7294 - accuracy: 0.6960 - val_loss: 0.6706 - val_accuracy: 0.7219 - lr: 2.0000e-05\nEpoch 8/30\n93/93 [==============================] - 24s 258ms/step - loss: 0.6924 - accuracy: 0.7084 - val_loss: 0.6682 - val_accuracy: 0.7181 - lr: 2.0000e-05\nEpoch 9/30\n93/93 [==============================] - 24s 255ms/step - loss: 0.6880 - accuracy: 0.7034 - val_loss: 0.6620 - val_accuracy: 0.7219 - lr: 2.0000e-05\nEpoch 10/30\n93/93 [==============================] - 24s 260ms/step - loss: 0.6771 - accuracy: 0.7222 - val_loss: 0.6560 - val_accuracy: 0.7276 - lr: 2.0000e-05\nEpoch 11/30\n93/93 [==============================] - 24s 261ms/step - loss: 0.6652 - accuracy: 0.7279 - val_loss: 0.6588 - val_accuracy: 0.7333 - lr: 2.0000e-05\nEpoch 12/30\n93/93 [==============================] - 24s 259ms/step - loss: 0.6607 - accuracy: 0.7296 - val_loss: 0.6549 - val_accuracy: 0.7276 - lr: 2.0000e-05\nEpoch 13/30\n93/93 [==============================] - 24s 259ms/step - loss: 0.6416 - accuracy: 0.7300 - val_loss: 0.6528 - val_accuracy: 0.7314 - lr: 2.0000e-05\nEpoch 14/30\n93/93 [==============================] - 24s 260ms/step - loss: 0.6413 - accuracy: 0.7384 - val_loss: 0.6458 - val_accuracy: 0.7295 - lr: 2.0000e-05\nProcessing test data...\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 11s 208ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# CLIP without early stopping\nEpoch: 30\\\nAccuracy: 0.7929\\\nVal. accuracy: 0.7371\\\nLayers: 512->256\\\nBatch size: 32","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T08:05:39.434689Z","iopub.execute_input":"2025-01-09T08:05:39.435060Z","iopub.status.idle":"2025-01-09T08:22:16.467403Z","shell.execute_reply.started":"2025-01-09T08:05:39.435031Z","shell.execute_reply":"2025-01-09T08:22:16.466453Z"}},"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\nProcessing validation data...\nEpoch 1/30\n93/93 [==============================] - 51s 294ms/step - loss: 0.9177 - accuracy: 0.5865 - val_loss: 0.7849 - val_accuracy: 0.6457\nEpoch 2/30\n93/93 [==============================] - 21s 231ms/step - loss: 0.8502 - accuracy: 0.6148 - val_loss: 0.7463 - val_accuracy: 0.6971\nEpoch 3/30\n93/93 [==============================] - 22s 237ms/step - loss: 0.8175 - accuracy: 0.6434 - val_loss: 0.7230 - val_accuracy: 0.6952\nEpoch 4/30\n93/93 [==============================] - 22s 240ms/step - loss: 0.7851 - accuracy: 0.6529 - val_loss: 0.7041 - val_accuracy: 0.7200\nEpoch 5/30\n93/93 [==============================] - 22s 237ms/step - loss: 0.7577 - accuracy: 0.6781 - val_loss: 0.6929 - val_accuracy: 0.7276\nEpoch 6/30\n93/93 [==============================] - 22s 237ms/step - loss: 0.7331 - accuracy: 0.6832 - val_loss: 0.6867 - val_accuracy: 0.7314\nEpoch 7/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.7134 - accuracy: 0.7003 - val_loss: 0.6811 - val_accuracy: 0.7295\nEpoch 8/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6856 - accuracy: 0.7064 - val_loss: 0.6791 - val_accuracy: 0.7200\nEpoch 9/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6863 - accuracy: 0.7077 - val_loss: 0.6748 - val_accuracy: 0.7333\nEpoch 10/30\n93/93 [==============================] - 22s 237ms/step - loss: 0.6713 - accuracy: 0.7236 - val_loss: 0.6744 - val_accuracy: 0.7352\nEpoch 11/30\n93/93 [==============================] - 22s 237ms/step - loss: 0.6666 - accuracy: 0.7202 - val_loss: 0.6704 - val_accuracy: 0.7219\nEpoch 12/30\n93/93 [==============================] - 22s 237ms/step - loss: 0.6513 - accuracy: 0.7337 - val_loss: 0.6695 - val_accuracy: 0.7352\nEpoch 13/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6461 - accuracy: 0.7303 - val_loss: 0.6661 - val_accuracy: 0.7390\nEpoch 14/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6241 - accuracy: 0.7451 - val_loss: 0.6618 - val_accuracy: 0.7390\nEpoch 15/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6361 - accuracy: 0.7350 - val_loss: 0.6581 - val_accuracy: 0.7429\nEpoch 16/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6008 - accuracy: 0.7471 - val_loss: 0.6587 - val_accuracy: 0.7352\nEpoch 17/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6113 - accuracy: 0.7468 - val_loss: 0.6565 - val_accuracy: 0.7276\nEpoch 18/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.6100 - accuracy: 0.7411 - val_loss: 0.6558 - val_accuracy: 0.7371\nEpoch 19/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5983 - accuracy: 0.7492 - val_loss: 0.6581 - val_accuracy: 0.7410\nEpoch 20/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5875 - accuracy: 0.7599 - val_loss: 0.6556 - val_accuracy: 0.7448\nEpoch 21/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5736 - accuracy: 0.7596 - val_loss: 0.6601 - val_accuracy: 0.7371\nEpoch 22/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5830 - accuracy: 0.7603 - val_loss: 0.6540 - val_accuracy: 0.7352\nEpoch 23/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5614 - accuracy: 0.7707 - val_loss: 0.6542 - val_accuracy: 0.7390\nEpoch 24/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5722 - accuracy: 0.7545 - val_loss: 0.6559 - val_accuracy: 0.7371\nEpoch 25/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5456 - accuracy: 0.7754 - val_loss: 0.6570 - val_accuracy: 0.7429\nEpoch 26/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5422 - accuracy: 0.7758 - val_loss: 0.6531 - val_accuracy: 0.7371\nEpoch 27/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5322 - accuracy: 0.7811 - val_loss: 0.6549 - val_accuracy: 0.7352\nEpoch 28/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5160 - accuracy: 0.7896 - val_loss: 0.6590 - val_accuracy: 0.7352\nEpoch 29/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5312 - accuracy: 0.7848 - val_loss: 0.6563 - val_accuracy: 0.7333\nEpoch 30/30\n93/93 [==============================] - 22s 238ms/step - loss: 0.5086 - accuracy: 0.7929 - val_loss: 0.6571 - val_accuracy: 0.7371\nProcessing test data...\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 11s 192ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# CLIP without early stopping v-2.0\nEpoch: 20\\\nAccuracy: 0.7535\\\nVal. accuracy: 0.7295\\\nLayers: 256->128\\\nBatch size: 16","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(256, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T08:47:41.017400Z","iopub.execute_input":"2025-01-09T08:47:41.017792Z","iopub.status.idle":"2025-01-09T09:00:45.160275Z","shell.execute_reply.started":"2025-01-09T08:47:41.017713Z","shell.execute_reply":"2025-01-09T09:00:45.159287Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38f46cc04bf46af80fe59b22d04002b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e427fa502b67408ba62fb79b8138b954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/842k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8490f48a12eb47bfb3b557d4141d7b31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/512k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5128e01d9ef94003b0f5c5fd90539340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d0c2f21a00494e931a08f1dd5293eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b026223ce004ef193d889e1a00d8477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0160a5fe19a44d3296864d36bf6554da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/577M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"952fd3074dcf436cba306ab11e5cd2d1"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\nProcessing validation data...\nEpoch 1/20\n186/186 [==============================] - 46s 142ms/step - loss: 1.0592 - accuracy: 0.4993 - val_loss: 0.8441 - val_accuracy: 0.6171\nEpoch 2/20\n186/186 [==============================] - 21s 113ms/step - loss: 0.8901 - accuracy: 0.5822 - val_loss: 0.7870 - val_accuracy: 0.6476\nEpoch 3/20\n186/186 [==============================] - 21s 115ms/step - loss: 0.8346 - accuracy: 0.6182 - val_loss: 0.7630 - val_accuracy: 0.6667\nEpoch 4/20\n186/186 [==============================] - 22s 118ms/step - loss: 0.7992 - accuracy: 0.6492 - val_loss: 0.7395 - val_accuracy: 0.7029\nEpoch 5/20\n186/186 [==============================] - 22s 120ms/step - loss: 0.7769 - accuracy: 0.6636 - val_loss: 0.7269 - val_accuracy: 0.6971\nEpoch 6/20\n186/186 [==============================] - 22s 121ms/step - loss: 0.7570 - accuracy: 0.6774 - val_loss: 0.7148 - val_accuracy: 0.7124\nEpoch 7/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.7498 - accuracy: 0.6795 - val_loss: 0.7069 - val_accuracy: 0.7219\nEpoch 8/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.7332 - accuracy: 0.6852 - val_loss: 0.6991 - val_accuracy: 0.7105\nEpoch 9/20\n186/186 [==============================] - 22s 120ms/step - loss: 0.7105 - accuracy: 0.6912 - val_loss: 0.6901 - val_accuracy: 0.7295\nEpoch 10/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.7042 - accuracy: 0.7047 - val_loss: 0.6841 - val_accuracy: 0.7333\nEpoch 11/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6913 - accuracy: 0.7148 - val_loss: 0.6832 - val_accuracy: 0.7257\nEpoch 12/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6763 - accuracy: 0.7226 - val_loss: 0.6824 - val_accuracy: 0.7162\nEpoch 13/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6775 - accuracy: 0.7215 - val_loss: 0.6751 - val_accuracy: 0.7295\nEpoch 14/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6540 - accuracy: 0.7300 - val_loss: 0.6757 - val_accuracy: 0.7181\nEpoch 15/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6535 - accuracy: 0.7236 - val_loss: 0.6683 - val_accuracy: 0.7219\nEpoch 16/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6424 - accuracy: 0.7316 - val_loss: 0.6688 - val_accuracy: 0.7200\nEpoch 17/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6227 - accuracy: 0.7508 - val_loss: 0.6723 - val_accuracy: 0.7238\nEpoch 18/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6188 - accuracy: 0.7461 - val_loss: 0.6687 - val_accuracy: 0.7352\nEpoch 19/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6176 - accuracy: 0.7498 - val_loss: 0.6664 - val_accuracy: 0.7314\nEpoch 20/20\n186/186 [==============================] - 22s 119ms/step - loss: 0.6049 - accuracy: 0.7535 - val_loss: 0.6728 - val_accuracy: 0.7295\nProcessing test data...\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 11s 183ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# CLIP modified\n\nEpoch: 20\\\nAccuracy: 0.7576\\\nVal. accuracy: 0.7371\\\nLayers: 512->256\\\nBatch size: 16\\\nDropout: 0.4","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T13:18:51.713664Z","iopub.execute_input":"2025-01-09T13:18:51.714058Z","iopub.status.idle":"2025-01-09T13:32:42.382109Z","shell.execute_reply.started":"2025-01-09T13:18:51.713972Z","shell.execute_reply":"2025-01-09T13:32:42.381137Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f35d94f32d44ad7b2f0cc4978d5f3f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc9ead50f9b46148262f06f5f7a1a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/842k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe129df20e54bdab144bcad1727483f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/512k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d87edb9194654b718ca84de39b8da9dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3acdf16e5c564778b92034acb5c4a3c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13987bd0bb404998b038177f606ad767"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499c24efb171434ba19632c2fb015851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/577M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93c7cea6e180462a8eab4251ca582d6b"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\nProcessing validation data...\nEpoch 1/20\n186/186 [==============================] - 48s 144ms/step - loss: 0.9624 - accuracy: 0.5539 - val_loss: 0.7995 - val_accuracy: 0.6514\nEpoch 2/20\n186/186 [==============================] - 21s 114ms/step - loss: 0.8702 - accuracy: 0.6104 - val_loss: 0.7469 - val_accuracy: 0.7048\nEpoch 3/20\n186/186 [==============================] - 22s 116ms/step - loss: 0.8252 - accuracy: 0.6300 - val_loss: 0.7304 - val_accuracy: 0.7010\nEpoch 4/20\n186/186 [==============================] - 22s 118ms/step - loss: 0.7986 - accuracy: 0.6441 - val_loss: 0.7148 - val_accuracy: 0.7067\nEpoch 5/20\n186/186 [==============================] - 22s 118ms/step - loss: 0.7554 - accuracy: 0.6747 - val_loss: 0.7105 - val_accuracy: 0.7086\nEpoch 6/20\n186/186 [==============================] - 22s 120ms/step - loss: 0.7415 - accuracy: 0.6966 - val_loss: 0.6968 - val_accuracy: 0.7124\nEpoch 7/20\n186/186 [==============================] - 23s 121ms/step - loss: 0.7299 - accuracy: 0.6916 - val_loss: 0.6932 - val_accuracy: 0.7143\nEpoch 8/20\n186/186 [==============================] - 23s 123ms/step - loss: 0.7152 - accuracy: 0.6875 - val_loss: 0.6872 - val_accuracy: 0.7143\nEpoch 9/20\n186/186 [==============================] - 23s 123ms/step - loss: 0.7070 - accuracy: 0.6970 - val_loss: 0.6852 - val_accuracy: 0.7124\nEpoch 10/20\n186/186 [==============================] - 23s 123ms/step - loss: 0.6818 - accuracy: 0.7199 - val_loss: 0.6864 - val_accuracy: 0.7200\nEpoch 11/20\n186/186 [==============================] - 23s 123ms/step - loss: 0.6544 - accuracy: 0.7239 - val_loss: 0.6683 - val_accuracy: 0.7124\nEpoch 12/20\n186/186 [==============================] - 23s 123ms/step - loss: 0.6600 - accuracy: 0.7232 - val_loss: 0.6732 - val_accuracy: 0.7314\nEpoch 13/20\n186/186 [==============================] - 23s 124ms/step - loss: 0.6537 - accuracy: 0.7182 - val_loss: 0.6665 - val_accuracy: 0.7295\nEpoch 14/20\n186/186 [==============================] - 23s 124ms/step - loss: 0.6407 - accuracy: 0.7350 - val_loss: 0.6666 - val_accuracy: 0.7257\nEpoch 15/20\n186/186 [==============================] - 23s 124ms/step - loss: 0.6108 - accuracy: 0.7478 - val_loss: 0.6595 - val_accuracy: 0.7295\nEpoch 16/20\n186/186 [==============================] - 23s 123ms/step - loss: 0.6123 - accuracy: 0.7414 - val_loss: 0.6636 - val_accuracy: 0.7257\nEpoch 17/20\n186/186 [==============================] - 23s 124ms/step - loss: 0.5943 - accuracy: 0.7539 - val_loss: 0.6597 - val_accuracy: 0.7352\nEpoch 18/20\n186/186 [==============================] - 23s 124ms/step - loss: 0.5865 - accuracy: 0.7589 - val_loss: 0.6686 - val_accuracy: 0.7314\nEpoch 19/20\n186/186 [==============================] - 23s 124ms/step - loss: 0.5865 - accuracy: 0.7549 - val_loss: 0.6602 - val_accuracy: 0.7333\nEpoch 20/20\n186/186 [==============================] - 23s 124ms/step - loss: 0.5824 - accuracy: 0.7576 - val_loss: 0.6585 - val_accuracy: 0.7371\nProcessing test data...\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 11s 192ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# FLAVA model\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import FlavaProcessor, FlavaModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nimport torch.nn.functional as F\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass SentimentDataset(Dataset):\n    def __init__(self, image_paths, texts, labels=None, processor=None):\n        self.image_paths = image_paths\n        self.texts = texts\n        self.labels = labels\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load and process image\n        try:\n            image = Image.open(self.image_paths[idx]).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {self.image_paths[idx]}: {str(e)}\")\n            image = Image.new('RGB', (224, 224), color='black')\n\n        # Process inputs\n        inputs = self.processor(\n            images=image,\n            text=self.texts[idx],\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True\n        )\n\n        # Remove batch dimension added by processor\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n\n        if self.labels is not None:\n            inputs['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n\n        return inputs\n\nclass FLAVASentimentModel(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        self.num_classes = num_classes\n        self.flava = FlavaModel.from_pretrained(\"facebook/flava-full\")\n        \n        # Freeze FLAVA layers\n        for param in self.flava.parameters():\n            param.requires_grad = False\n            \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(self.flava.config.hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        outputs = self.flava(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Get multimodal embeddings\n        multimodal_embeddings = outputs.multimodal_embeddings[:, 0]  # Use [CLS] token\n        logits = self.classifier(multimodal_embeddings)\n        return logits\n\ndef train_model(train_loader, val_loader, device, epochs=30):\n    model = FLAVASentimentModel().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.2, patience=2, min_lr=1e-6\n    )\n    \n    best_val_acc = 0\n    best_model = None\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask, pixel_values)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100. * correct / total\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                pixel_values = batch['pixel_values'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask, pixel_values)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_acc = 100. * correct / total\n        scheduler.step(val_acc)\n        \n        print(f'Epoch {epoch + 1}: Train Loss: {train_loss/len(train_loader):.3f}, '\n              f'Train Acc: {train_acc:.3f}%, Val Loss: {val_loss/len(val_loader):.3f}, '\n              f'Val Acc: {val_acc:.3f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model = model.state_dict().copy()\n    \n    # Load best model\n    model.load_state_dict(best_model)\n    return model\n\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Initialize processor\n    processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n    \n    # Create datasets\n    train_dataset = SentimentDataset(train_paths, train_texts, train_labs, processor)\n    val_dataset = SentimentDataset(val_paths, val_texts, val_labs, processor)\n    test_dataset = SentimentDataset(test_image_paths, test_df['Captions'].tolist(), None, processor)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32)\n    test_loader = DataLoader(test_dataset, batch_size=32)\n    \n    # Train model\n    model = train_model(train_loader, val_loader, device)\n    \n    # Make predictions\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            \n            outputs = model(input_ids, attention_mask, pixel_values)\n            _, predicted = outputs.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predictions]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:23:11.848241Z","iopub.execute_input":"2025-01-09T10:23:11.848715Z","iopub.status.idle":"2025-01-09T10:23:19.238211Z","shell.execute_reply.started":"2025-01-09T10:23:11.848679Z","shell.execute_reply":"2025-01-09T10:23:19.236298Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.output.conv.weight', 'mlm_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'itm_head.seq_relationship.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'mmm_image_head.decoder.weight', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'mmm_image_head.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'mim_head.bias', 'mmm_image_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'mim_head.decoder.weight', 'mmm_image_head.transform.dense.weight', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'mmm_text_head.decoder.bias', 'mmm_image_head.transform.LayerNorm.weight', 'mmm_text_head.transform.dense.bias', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'mlm_head.decoder.weight', 'mmm_text_head.bias', 'mim_head.decoder.bias', 'mlm_head.transform.dense.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'mlm_head.bias', 'mmm_text_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'itm_head.seq_relationship.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'mim_head.transform.dense.bias', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'mmm_text_head.decoder.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias']\n- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nEpoch 1/30:   0%|          | 0/93 [00:02<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/361800475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/361800475.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/361800475.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, device, epochs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [42] at entry 0 and [14] at entry 1"],"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [42] at entry 0 and [14] at entry 1","output_type":"error"}],"execution_count":16}]}