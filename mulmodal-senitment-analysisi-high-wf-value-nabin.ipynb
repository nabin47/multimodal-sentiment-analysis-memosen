{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":46125,"databundleVersionId":4972941,"sourceType":"competition"}],"dockerImageVersionId":30397,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center;font-weight: 900; font-size:40px;\"> Multimodal Sentiment Analysis Higher Accuracy </p>","metadata":{}},{"cell_type":"markdown","source":"**More robust vgg19 and xlm-roberta******","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport os\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=3,\n            min_lr=1e-6\n        )\n    ]\n\n    # Data augmentation for images\n    train_images_augmented = augment_images(train_images)\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:24:30.074829Z","iopub.execute_input":"2025-01-06T18:24:30.075204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nimport os\nimport math\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef cosine_decay(epoch):\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(epoch * math.pi / 20)) / 2\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=2,  # Stop earlier\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(cosine_decay)\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:11:00.158581Z","iopub.execute_input":"2025-01-06T18:11:00.158941Z","iopub.status.idle":"2025-01-06T18:22:16.469812Z","shell.execute_reply.started":"2025-01-06T18:11:00.158912Z","shell.execute_reply":"2025-01-06T18:22:16.46908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This vgg19 and bert model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:01:08.238623Z","iopub.execute_input":"2025-01-06T17:01:08.239137Z","execution_failed":"2025-01-06T17:34:25.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:28:02.213845Z","iopub.execute_input":"2025-01-06T15:28:02.214167Z","iopub.status.idle":"2025-01-06T16:38:51.269392Z","shell.execute_reply.started":"2025-01-06T15:28:02.214142Z","shell.execute_reply":"2025-01-06T16:38:51.268536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=10):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:11:27.045036Z","iopub.execute_input":"2025-01-06T15:11:27.045356Z","iopub.status.idle":"2025-01-06T15:27:46.780771Z","shell.execute_reply.started":"2025-01-06T15:11:27.045331Z","shell.execute_reply":"2025-01-06T15:27:46.779955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T10:43:23.668372Z","iopub.execute_input":"2025-01-06T10:43:23.6687Z","iopub.status.idle":"2025-01-06T11:36:22.03778Z","shell.execute_reply.started":"2025-01-06T10:43:23.668674Z","shell.execute_reply":"2025-01-06T11:36:22.03688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Less layer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T11:41:58.166279Z","iopub.execute_input":"2025-01-06T11:41:58.166641Z","iopub.status.idle":"2025-01-06T12:34:48.887549Z","shell.execute_reply.started":"2025-01-06T11:41:58.166615Z","shell.execute_reply":"2025-01-06T12:34:48.886596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:13:06.320848Z","iopub.execute_input":"2025-01-06T14:13:06.321144Z","iopub.status.idle":"2025-01-06T15:03:11.81315Z","shell.execute_reply.started":"2025-01-06T14:13:06.321084Z","shell.execute_reply":"2025-01-06T15:03:11.812201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Edited","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\n\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs,\n        class_weights_dict\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Evaluate model\n    print(classification_report(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n    print(confusion_matrix(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n\n    # Save predictions\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:41:14.208845Z","iopub.execute_input":"2025-01-06T16:41:14.209344Z","iopub.status.idle":"2025-01-06T16:52:55.961419Z","shell.execute_reply.started":"2025-01-06T16:41:14.209314Z","shell.execute_reply":"2025-01-06T16:52:55.960223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\nimport tensorflow_addons as tfa\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Focal loss for better handling of class imbalance\n    loss = tfa.losses.SigmoidFocalCrossEntropy()\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss=loss,\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:38:09.070207Z","iopub.execute_input":"2025-01-06T18:38:09.070634Z","iopub.status.idle":"2025-01-06T18:41:26.481732Z","shell.execute_reply.started":"2025-01-06T18:38:09.070548Z","shell.execute_reply":"2025-01-06T18:41:26.480188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Convert labels to one-hot encoding\n    train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n    val_labels_onehot = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels_onehot,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels_onehot\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:47:32.681831Z","iopub.execute_input":"2025-01-06T18:47:32.682182Z","iopub.status.idle":"2025-01-06T18:59:22.905496Z","shell.execute_reply.started":"2025-01-06T18:47:32.682107Z","shell.execute_reply":"2025-01-06T18:59:22.904502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T05:53:13.106974Z","iopub.execute_input":"2025-01-07T05:53:13.107721Z","iopub.status.idle":"2025-01-07T05:53:32.898832Z","shell.execute_reply.started":"2025-01-07T05:53:13.107689Z","shell.execute_reply":"2025-01-07T05:53:32.8977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu')(x)  # Add Dense layer instead of pooling\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T06:13:41.793368Z","iopub.execute_input":"2025-01-07T06:13:41.79381Z","iopub.status.idle":"2025-01-07T06:45:12.973797Z","shell.execute_reply.started":"2025-01-07T06:13:41.793732Z","shell.execute_reply":"2025-01-07T06:45:12.972784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:33:48.16119Z","iopub.execute_input":"2025-01-07T08:33:48.161544Z","iopub.status.idle":"2025-01-07T08:34:09.481692Z","shell.execute_reply.started":"2025-01-07T08:33:48.161516Z","shell.execute_reply":"2025-01-07T08:34:09.480417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',  # Monitor validation loss\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,  # Decreased batch size for better generalization\n        class_weight=class_weight_dict,  # Add class weights\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:34:31.838915Z","iopub.execute_input":"2025-01-07T08:34:31.839388Z","execution_failed":"2025-01-07T09:14:48.518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow pandas numpy tqdm scikit-learn transformers vit-keras\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:17:53.255584Z","iopub.execute_input":"2025-01-07T09:17:53.255924Z","iopub.status.idle":"2025-01-07T09:18:13.372949Z","shell.execute_reply.started":"2025-01-07T09:17:53.2559Z","shell.execute_reply":"2025-01-07T09:18:13.371612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import LabelEncoder\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n# Image augmentation\ndef augment_images(images):\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=len(images), shuffle=False).next()\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Augment training images\n    train_images = augment_images(train_images)\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        class_weight=class_weight_dict,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:18:28.830186Z","iopub.execute_input":"2025-01-07T09:18:28.830493Z","execution_failed":"2025-01-07T09:36:14.932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VGG19 + BERT with Regularizaition and 0.4 dropout \nAdded regularization and high dropout rate to avoid overfitting.\nBatch size: 32\nLayer: 256","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\n\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs,\n        class_weights_dict\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Evaluate model\n    print(classification_report(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n    print(confusion_matrix(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n\n    # Save predictions\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VisualBERT multimodal model default settings","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import TFVisualBertModel, VisualBertConfig, BertTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport torch\nfrom torchvision.models import resnet50\nfrom torchvision.transforms import Compose, Resize, ToTensor, Normalize\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, transform):\n    \"\"\"Load and preprocess a single image using PyTorch transforms\"\"\"\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return torch.zeros((3, 224, 224))\n\ndef process_images(image_paths):\n    \"\"\"Process all images and extract features using ResNet\"\"\"\n    transform = Compose([\n        Resize((224, 224)),\n        ToTensor(),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load ResNet model\n    resnet = resnet50(pretrained=True)\n    resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # Remove classification layer\n    resnet.eval()\n    \n    features = []\n    with torch.no_grad():\n        for path in tqdm(image_paths, desc=\"Processing images\"):\n            img = preprocess_image(path, transform)\n            img = img.unsqueeze(0)  # Add batch dimension\n            feat = resnet(img)\n            features.append(feat.squeeze().numpy())\n    \n    return np.array(features)\n\nclass VisualBertSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.config = VisualBertConfig.from_pretrained('uclanlp/visualbert-vqa')\n        self.visual_bert = TFVisualBertModel.from_pretrained('uclanlp/visualbert-vqa', config=self.config)\n        \n    def build_model(self):\n        # Inputs\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        visual_features = Input(shape=(2048,), dtype=tf.float32, name='visual_features')\n        visual_attention_mask = Input(shape=(1,), dtype=tf.int32, name='visual_attention_mask')\n        \n        # Expand visual features dimensions to match VisualBERT requirements\n        visual_embeds = tf.expand_dims(visual_features, axis=1)  # Replace unsqueeze with expand_dims\n        \n        # Pass through VisualBERT\n        outputs = self.visual_bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            visual_embeddings=visual_embeds,\n            visual_attention_mask=visual_attention_mask\n        )\n        \n        # Get pooled output\n        pooled_output = outputs[1]\n        \n        # Classification layers\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, visual_features, visual_attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = VisualBertSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Prepare visual attention masks\n    train_visual_attention_mask = tf.ones((train_images.shape[0], 1), dtype=tf.int32)\n    val_visual_attention_mask = tf.ones((val_images.shape[0], 1), dtype=tf.int32)\n    \n    # Convert images to float32\n    train_images = tf.cast(train_images, tf.float32)\n    val_images = tf.cast(val_images, tf.float32)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask,\n            'visual_features': train_images,\n            'visual_attention_mask': train_visual_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask,\n                'visual_features': val_images,\n                'visual_attention_mask': val_visual_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths and extract features\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    train_features = process_images(train_image_paths)\n    test_features = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_feats, val_feats, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_features, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_feats, train_texts, train_labs,\n        val_feats, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = VisualBertSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    test_visual_attention_mask = tf.ones((test_features.shape[0], 1), dtype=tf.int32)\n    test_features = tf.cast(test_features, tf.float32)\n    \n    # Make predictions\n    predictions = model.predict({\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask,\n        'visual_features': test_features,\n        'visual_attention_mask': test_visual_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:18:54.091720Z","iopub.execute_input":"2025-01-09T07:18:54.092139Z","iopub.status.idle":"2025-01-09T07:18:54.141985Z","shell.execute_reply.started":"2025-01-09T07:18:54.092108Z","shell.execute_reply":"2025-01-09T07:18:54.140727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP model with early stopping\n\nStopped at: epoch 14\nEpoch: 30\\\nAccuracy: 0.7384\\\nVal. accuracy: 0.7295\\\nLayers: 512->256\\\nBatch size: 32","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:20:30.294262Z","iopub.execute_input":"2025-01-09T07:20:30.294642Z","iopub.status.idle":"2025-01-09T07:31:17.890927Z","shell.execute_reply.started":"2025-01-09T07:20:30.294610Z","shell.execute_reply":"2025-01-09T07:31:17.889953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping\nEpoch: 30\\\nAccuracy: 0.7929\\\nVal. accuracy: 0.7371\\\nLayers: 512->256\\\nBatch size: 32","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T08:05:39.434689Z","iopub.execute_input":"2025-01-09T08:05:39.435060Z","iopub.status.idle":"2025-01-09T08:22:16.467403Z","shell.execute_reply.started":"2025-01-09T08:05:39.435031Z","shell.execute_reply":"2025-01-09T08:22:16.466453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP without early stopping v-2.0\nEpoch: 20\\\nAccuracy: 0.7535\\\nVal. accuracy: 0.7295\\\nLayers: 256->128\\\nBatch size: 16","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(256, activation='relu')(combined_features)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T08:47:41.017400Z","iopub.execute_input":"2025-01-09T08:47:41.017792Z","iopub.status.idle":"2025-01-09T09:00:45.160275Z","shell.execute_reply.started":"2025-01-09T08:47:41.017713Z","shell.execute_reply":"2025-01-09T09:00:45.159287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-3\n## Increased dropout, decreased batch size\nEpoch: 20\\\nAccuracy: 0.7576\\\nVal. accuracy: 0.7371\\\nLayers: 512->256\\\nBatch size: 16\\\nDropout: 0.4","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu')(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T13:18:51.713664Z","iopub.execute_input":"2025-01-09T13:18:51.714058Z","iopub.status.idle":"2025-01-09T13:32:42.382109Z","shell.execute_reply.started":"2025-01-09T13:18:51.713972Z","shell.execute_reply":"2025-01-09T13:32:42.381137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-4\n## Added kernel regularizer\nEpoch: 20\\\nAccuracy: 0.7613\\\nVal. accuracy: 0.7390\\\nLayers: 512->256\\\nBatch size: 16\\\nDropout: 0.4","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=16,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T13:47:49.471280Z","iopub.execute_input":"2025-01-09T13:47:49.471670Z","iopub.status.idle":"2025-01-09T14:00:21.073949Z","shell.execute_reply.started":"2025-01-09T13:47:49.471624Z","shell.execute_reply":"2025-01-09T14:00:21.073027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-5\n## Increase batch size and decay\nEpoch: 30\\\nAccuracy: 0.7613\\\nVal. accuracy: 0.7390\\\nLayers: 512->256\\\nBatch size: 32\\\nDropout: 0.4\\\nRegularization decay: 0.2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:31:30.678232Z","iopub.execute_input":"2025-01-09T14:31:30.678548Z","iopub.status.idle":"2025-01-09T14:54:29.868943Z","shell.execute_reply.started":"2025-01-09T14:31:30.678522Z","shell.execute_reply":"2025-01-09T14:54:29.867954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-6\n## Same as v-5 but increased dropout in two layers instead of just one\nEpoch: 50\\\nAccuracy: 0.7818\\\nVal. accuracy: 0.7486\\\nLayers: 512->256\\\nBatch size: 32\\\nDropout: 0.4\\\nRegularization decay: 0.2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(x)\n        x = Dropout(0.4)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=50):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:02:28.814112Z","iopub.execute_input":"2025-01-09T15:02:28.814733Z","iopub.status.idle":"2025-01-09T15:25:28.566506Z","shell.execute_reply.started":"2025-01-09T15:02:28.814671Z","shell.execute_reply":"2025-01-09T15:25:28.565374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP modified v-7\n## Same as v-6 but used 30 epochs\nEpoch: 30\\\nAccuracy: 0.7613\\\nVal. accuracy: 0.7390\\\nLayers: 512->256\\\nBatch size: 32\\\nDropout: 0.4\\\nRegularization decay: 0.2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom transformers import CLIPProcessor, TFCLIPModel\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LayerNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\nclass CLIPSentimentModel:\n    def __init__(self, num_classes=3):\n        self.num_classes = num_classes\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP layers\n        self.clip.trainable = False\n    \n    def process_batch(self, images, texts):\n        \"\"\"Process a batch of images and texts through CLIP\"\"\"\n        inputs = self.processor(\n            images=images,\n            text=texts,\n            return_tensors=\"tf\",\n            padding=True,\n            truncation=True\n        )\n        return inputs\n    \n    def build_model(self):\n        # Define inputs\n        input_ids = Input(shape=(77,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(77,), dtype=tf.int32, name='attention_mask')\n        pixel_values = Input(shape=(3, 224, 224), dtype=tf.float32, name='pixel_values')\n        \n        # Get CLIP embeddings\n        clip_outputs = self.clip(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values\n        )\n        \n        # Concatenate image and text features\n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        combined_features = Concatenate()([image_features, text_features])\n        \n        # Classification head\n        x = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(combined_features)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=l2(0.02))(x)\n        x = Dropout(0.4)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[input_ids, attention_mask, pixel_values],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n\ndef process_data_batch(image_paths, texts, model_handler, batch_size=32):\n    \"\"\"Process data in batches to avoid memory issues\"\"\"\n    all_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        # Load images for current batch\n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                # Create a blank image as fallback\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        # Process batch\n        inputs = model_handler.process_batch(batch_images, batch_texts)\n        all_inputs.append(inputs)\n    \n    # Combine all batches\n    combined_inputs = {\n        'input_ids': tf.concat([x['input_ids'] for x in all_inputs], axis=0),\n        'attention_mask': tf.concat([x['attention_mask'] for x in all_inputs], axis=0),\n        'pixel_values': tf.concat([x['pixel_values'] for x in all_inputs], axis=0)\n    }\n    \n    return combined_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = CLIPSentimentModel()\n    model = model_handler.build_model()\n    \n    print(\"Processing training data...\")\n    train_inputs = process_data_batch(train_image_paths, train_texts, model_handler)\n    \n    print(\"Processing validation data...\")\n    val_inputs = process_data_batch(val_image_paths, val_texts, model_handler)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_inputs,\n        train_labels,\n        validation_data=(val_inputs, val_labels),\n        epochs=epochs,\n        batch_size=32,\n        # callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs\n    )\n    \n    # Process test data\n    print(\"Processing test data...\")\n    model_handler = CLIPSentimentModel()\n    test_inputs = process_data_batch(test_image_paths, test_df['Captions'].tolist(), model_handler)\n    \n    # Make predictions\n    predictions = model.predict(test_inputs)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:41:19.020378Z","iopub.execute_input":"2025-01-09T15:41:19.020720Z","iopub.status.idle":"2025-01-09T15:56:54.509250Z","shell.execute_reply.started":"2025-01-09T15:41:19.020694Z","shell.execute_reply":"2025-01-09T15:56:54.508268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FLAVA model v-1\nDefault FLAVA implemenation on the best result of CLIP\n\nEpoch: 30\\\nAccuracy: 74.92%\\\nVal. accuracy: 74.10%\\\nLayers: 512->512->512->256\\\nBatch size: 16\\\nDropout: 0.4\\\nWeight decay: 0.2","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T05:41:19.565927Z","iopub.execute_input":"2025-01-10T05:41:19.566297Z","iopub.status.idle":"2025-01-10T05:41:29.926771Z","shell.execute_reply.started":"2025-01-10T05:41:19.566266Z","shell.execute_reply":"2025-01-10T05:41:29.925507Z"}},"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.12.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import FlavaProcessor, FlavaModel\nfrom torch import nn\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nclass FLAVASentimentModel(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        self.num_classes = num_classes\n        self.processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n        self.flava = FlavaModel.from_pretrained(\"facebook/flava-full\")\n        \n        for param in self.flava.parameters():\n            param.requires_grad = False\n            \n        # Get multimodal output size from FLAVA config\n        hidden_size = self.flava.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, image_inputs, text_inputs):\n        outputs = self.flava(\n            input_ids=text_inputs['input_ids'],\n            attention_mask=text_inputs['attention_mask'],\n            pixel_values=image_inputs['pixel_values'],\n            return_dict=True\n        )\n        \n        # Get first token ([CLS]) of multimodal output\n        multimodal_output = outputs.multimodal_embeddings[:, 0]\n        return self.classifier(multimodal_output)\n\ndef process_data_batch(image_paths, texts, processor, batch_size=16, device='cuda'):\n    all_image_inputs = []\n    all_text_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        inputs = processor(\n            images=batch_images,\n            text=batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=77\n        )\n        \n        all_image_inputs.append({\n            'pixel_values': inputs['pixel_values'].to(device)\n        })\n        all_text_inputs.append({\n            'input_ids': inputs['input_ids'].to(device),\n            'attention_mask': inputs['attention_mask'].to(device)\n        })\n    \n    return all_image_inputs, all_text_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, \n                device='cuda', epochs=30, batch_size=16):\n    \n    model = FLAVASentimentModel().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.02)\n    criterion = nn.CrossEntropyLoss()\n    \n    print(\"Processing training data...\")\n    train_image_inputs, train_text_inputs = process_data_batch(\n        train_image_paths, train_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    print(\"Processing validation data...\")\n    val_image_inputs, val_text_inputs = process_data_batch(\n        val_image_paths, val_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    train_labels = torch.tensor(train_labels, device=device)\n    val_labels = torch.tensor(val_labels, device=device)\n    \n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for i in range(len(train_image_inputs)):\n            optimizer.zero_grad()\n            outputs = model(train_image_inputs[i], train_text_inputs[i])\n            batch_labels = train_labels[i * batch_size:(i + 1) * batch_size]\n            \n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += batch_labels.size(0)\n            correct += predicted.eq(batch_labels).sum().item()\n        \n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for i in range(len(val_image_inputs)):\n                outputs = model(val_image_inputs[i], val_text_inputs[i])\n                batch_labels = val_labels[i * batch_size:(i + 1) * batch_size]\n                \n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n        \n        val_acc = 100. * val_correct / val_total\n        print(f'Epoch {epoch + 1}: Train Acc: {100.*correct/total:.2f}% Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return model\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    batch_size = 16\n    \n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    model = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs,\n        device=device,\n        batch_size=batch_size\n    )\n    \n    print(\"Processing test data...\")\n    test_image_inputs, test_text_inputs = process_data_batch(\n        test_image_paths, test_df['Captions'].tolist(),\n        model.processor, batch_size=batch_size, device=device\n    )\n    \n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for i in range(len(test_image_inputs)):\n            outputs = model(test_image_inputs[i], test_text_inputs[i])\n            _, predicted = outputs.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predictions]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T04:35:15.371422Z","iopub.execute_input":"2025-01-10T04:35:15.371785Z","iopub.status.idle":"2025-01-10T05:28:37.364703Z","shell.execute_reply.started":"2025-01-10T04:35:15.371756Z","shell.execute_reply":"2025-01-10T05:28:37.363700Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'mmm_image_head.decoder.bias', 'itm_head.seq_relationship.bias', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'mmm_image_head.bias', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'itm_head.seq_relationship.weight', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'mim_head.transform.dense.bias', 'mmm_image_head.transform.dense.bias', 'mmm_image_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.input.bias', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'mmm_text_head.decoder.weight', 'mim_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'mlm_head.decoder.weight', 'mlm_head.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'mim_head.decoder.weight', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'mim_head.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'mmm_text_head.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'mmm_text_head.transform.LayerNorm.bias', 'mmm_text_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'mlm_head.transform.dense.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'mim_head.transform.LayerNorm.weight', 'mlm_head.transform.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'mmm_text_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias']\n- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\nProcessing validation data...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:813: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Acc: 61.72% Val Acc: 61.90%\nEpoch 2: Train Acc: 63.67% Val Acc: 62.86%\nEpoch 3: Train Acc: 63.23% Val Acc: 65.33%\nEpoch 4: Train Acc: 64.78% Val Acc: 65.14%\nEpoch 5: Train Acc: 64.21% Val Acc: 66.10%\nEpoch 6: Train Acc: 64.51% Val Acc: 67.24%\nEpoch 7: Train Acc: 66.36% Val Acc: 68.95%\nEpoch 8: Train Acc: 65.69% Val Acc: 67.62%\nEpoch 9: Train Acc: 66.09% Val Acc: 68.57%\nEpoch 10: Train Acc: 67.64% Val Acc: 69.33%\nEpoch 11: Train Acc: 67.37% Val Acc: 69.52%\nEpoch 12: Train Acc: 67.54% Val Acc: 70.29%\nEpoch 13: Train Acc: 68.75% Val Acc: 71.05%\nEpoch 14: Train Acc: 68.86% Val Acc: 71.43%\nEpoch 15: Train Acc: 69.16% Val Acc: 72.19%\nEpoch 16: Train Acc: 71.08% Val Acc: 71.62%\nEpoch 17: Train Acc: 70.71% Val Acc: 72.57%\nEpoch 18: Train Acc: 71.18% Val Acc: 72.38%\nEpoch 19: Train Acc: 71.35% Val Acc: 72.38%\nEpoch 20: Train Acc: 72.32% Val Acc: 73.71%\nEpoch 21: Train Acc: 72.22% Val Acc: 73.33%\nEpoch 22: Train Acc: 72.53% Val Acc: 73.71%\nEpoch 23: Train Acc: 73.27% Val Acc: 74.67%\nEpoch 24: Train Acc: 73.40% Val Acc: 75.05%\nEpoch 25: Train Acc: 73.00% Val Acc: 74.10%\nEpoch 26: Train Acc: 74.48% Val Acc: 74.67%\nEpoch 27: Train Acc: 73.74% Val Acc: 74.67%\nEpoch 28: Train Acc: 73.94% Val Acc: 73.90%\nEpoch 29: Train Acc: 75.29% Val Acc: 74.29%\nEpoch 30: Train Acc: 74.92% Val Acc: 74.10%\nProcessing test data...\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# BLIP-2 model v-1\nDefault BLIP-2 implemenation on the best result of CLIP\n\nEpoch: 30\\\nAccuracy: 74.92%\\\nVal. accuracy: 74.10%\\\nLayers: 512->512->512->256\\\nBatch size: 16\\\nDropout: 0.4\\\nWeight decay: 0.2","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers torch torchvision pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T05:53:17.349089Z","iopub.execute_input":"2025-01-10T05:53:17.349474Z","iopub.status.idle":"2025-01-10T05:53:48.845652Z","shell.execute_reply.started":"2025-01-10T05:53:17.349441Z","shell.execute_reply":"2025-01-10T05:53:48.844431Z"}},"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nCollecting transformers\n  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.12.0)\nCollecting torch\n  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.13.0)\nCollecting torchvision\n  Downloading torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (9.2.0)\nCollecting pillow\n  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nCollecting huggingface-hub<1.0,>=0.14.1\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting safetensors>=0.3.1\n  Downloading safetensors-0.5.2.tar.gz (66 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Cargo, the Rust package manager, is not installed or is not on PATH.\n  \u001b[31m   \u001b[0m This package requires Rust and Cargo to compile extensions. Install it through\n  \u001b[31m   \u001b[0m the system's package manager or via https://rustup.rs/\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Checking for Rust toolchain....\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m\u001b[0m Encountered error while generating package metadata.\n\u001b[31m>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoProcessor, BlipForConditionalGeneration\nfrom torch import nn\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nclass BLIPSentimentModel(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        self.num_classes = num_classes\n        self.processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        \n        for param in self.blip.parameters():\n            param.requires_grad = False\n            \n        self.classifier = nn.Sequential(\n            nn.Linear(768, 512),  # BLIP base hidden size is 768\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, image_inputs, text_inputs):\n        outputs = self.blip(\n            input_ids=text_inputs['input_ids'],\n            attention_mask=text_inputs['attention_mask'],\n            pixel_values=image_inputs['pixel_values'],\n            return_dict=True\n        )\n        \n        # Get image features\n        image_features = outputs.image_embeds[:, 0]  # [CLS] token\n        return self.classifier(image_features)\n\ndef process_data_batch(image_paths, texts, processor, batch_size=8, device='cuda'):\n    all_image_inputs = []\n    all_text_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        inputs = processor(\n            images=batch_images,\n            text=batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=77\n        )\n        \n        all_image_inputs.append({\n            'pixel_values': inputs['pixel_values'].to(device)\n        })\n        all_text_inputs.append({\n            'input_ids': inputs['input_ids'].to(device),\n            'attention_mask': inputs['attention_mask'].to(device)\n        })\n    \n    return all_image_inputs, all_text_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, \n                device='cuda', epochs=30, batch_size=8):\n    \n    print(\"Initializing model...\")\n    model = BLIPSentimentModel().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.02)\n    criterion = nn.CrossEntropyLoss()\n    \n    print(\"Processing training data...\")\n    train_image_inputs, train_text_inputs = process_data_batch(\n        train_image_paths, train_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    print(\"Processing validation data...\")\n    val_image_inputs, val_text_inputs = process_data_batch(\n        val_image_paths, val_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    train_labels = torch.tensor(train_labels, device=device)\n    val_labels = torch.tensor(val_labels, device=device)\n    \n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for i in range(len(train_image_inputs)):\n            optimizer.zero_grad()\n            outputs = model(train_image_inputs[i], train_text_inputs[i])\n            batch_labels = train_labels[i * batch_size:(i + 1) * batch_size]\n            \n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += batch_labels.size(0)\n            correct += predicted.eq(batch_labels).sum().item()\n        \n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for i in range(len(val_image_inputs)):\n                outputs = model(val_image_inputs[i], val_text_inputs[i])\n                batch_labels = val_labels[i * batch_size:(i + 1) * batch_size]\n                \n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n        \n        val_acc = 100. * val_correct / val_total\n        print(f'Epoch {epoch + 1}: Train Acc: {100.*correct/total:.2f}% Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return model\n\ndef main():\n    print(\"Installing required packages...\")\n    try:\n        import subprocess\n        subprocess.check_call([\"pip\", \"install\", \"--upgrade\", \"transformers\", \"torch\", \"torchvision\"])\n    except:\n        print(\"Warning: Could not install packages. Please ensure they are installed manually.\")\n        \n    batch_size = 8\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    print(\"Loading data...\")\n    train_df = pd.read_csv('/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv')\n    test_df = pd.read_csv('/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv')\n    \n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = [os.path.join(memes_folder, img) for img in train_df['image_name']]\n    test_image_paths = [os.path.join(memes_folder, img) for img in test_df['image_name']]\n    \n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    model = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs,\n        device=device,\n        batch_size=batch_size\n    )\n    \n    print(\"Processing test data...\")\n    test_image_inputs, test_text_inputs = process_data_batch(\n        test_image_paths, test_df['Captions'].tolist(),\n        model.processor, batch_size=batch_size, device=device\n    )\n    \n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for i in range(len(test_image_inputs)):\n            outputs = model(test_image_inputs[i], test_text_inputs[i])\n            _, predicted = outputs.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predictions]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T11:04:49.691943Z","iopub.execute_input":"2025-01-10T11:04:49.692326Z","iopub.status.idle":"2025-01-10T11:04:49.733838Z","shell.execute_reply.started":"2025-01-10T11:04:49.692296Z","shell.execute_reply":"2025-01-10T11:04:49.732757Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3439389725.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlipForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'BlipForConditionalGeneration' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'BlipForConditionalGeneration' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T11:06:38.847266Z","iopub.execute_input":"2025-01-10T11:06:38.847939Z","iopub.status.idle":"2025-01-10T11:06:38.853084Z","shell.execute_reply.started":"2025-01-10T11:06:38.847903Z","shell.execute_reply":"2025-01-10T11:06:38.852135Z"}},"outputs":[{"name":"stdout","text":"4.20.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# VILT model v-1 (Too memory hungry)\nDefault VILT implemenation","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import ViltProcessor, ViltModel\nfrom torch import nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport os\n\nclass VILTSentimentModel(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        self.num_classes = num_classes\n        self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n        self.vilt = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n        \n        for param in self.vilt.parameters():\n            param.requires_grad = False\n            \n        hidden_size = self.vilt.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, image_inputs, text_inputs):\n        outputs = self.vilt(\n            input_ids=text_inputs['input_ids'],\n            attention_mask=text_inputs['attention_mask'],\n            pixel_values=image_inputs['pixel_values'],\n            pixel_mask=image_inputs['pixel_mask'],\n            return_dict=True\n        )\n        \n        pooled_output = outputs.pooler_output\n        return self.classifier(pooled_output)\n\ndef process_data_batch(image_paths, texts, processor, batch_size=16, device='cuda'):\n    all_image_inputs = []\n    all_text_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = []\n        batch_texts = texts[i:i + batch_size]\n        \n        for path in image_paths[i:i + batch_size]:\n            try:\n                image = Image.open(path).convert('RGB')\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                batch_images.append(Image.new('RGB', (224, 224), color='black'))\n        \n        inputs = processor(\n            images=batch_images,\n            text=batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True\n        )\n        \n        all_image_inputs.append({\n            'pixel_values': inputs['pixel_values'].to(device),\n            'pixel_mask': inputs['pixel_mask'].to(device)\n        })\n        all_text_inputs.append({\n            'input_ids': inputs['input_ids'].to(device),\n            'attention_mask': inputs['attention_mask'].to(device)\n        })\n    \n    return all_image_inputs, all_text_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, \n                device='cuda', epochs=30, batch_size=16):\n    \n    model = VILTSentimentModel().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.02)\n    criterion = nn.CrossEntropyLoss()\n    \n    print(\"Processing training data...\")\n    train_image_inputs, train_text_inputs = process_data_batch(\n        train_image_paths, train_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    print(\"Processing validation data...\")\n    val_image_inputs, val_text_inputs = process_data_batch(\n        val_image_paths, val_texts, model.processor, batch_size=batch_size, device=device\n    )\n    \n    train_labels = torch.tensor(train_labels, device=device)\n    val_labels = torch.tensor(val_labels, device=device)\n    \n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for i in range(len(train_image_inputs)):\n            optimizer.zero_grad()\n            outputs = model(train_image_inputs[i], train_text_inputs[i])\n            batch_labels = train_labels[i * batch_size:(i + 1) * batch_size]\n            \n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += batch_labels.size(0)\n            correct += predicted.eq(batch_labels).sum().item()\n        \n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for i in range(len(val_image_inputs)):\n                outputs = model(val_image_inputs[i], val_text_inputs[i])\n                batch_labels = val_labels[i * batch_size:(i + 1) * batch_size]\n                \n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n        \n        val_acc = 100. * val_correct / val_total\n        print(f'Epoch {epoch + 1}: Train Acc: {100.*correct/total:.2f}% Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return model\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    batch_size = 16\n    \n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    model = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs,\n        device=device,\n        batch_size=batch_size\n    )\n    \n    print(\"Processing test data...\")\n    test_image_inputs, test_text_inputs = process_data_batch(\n        test_image_paths, test_df['Captions'].tolist(),\n        model.processor, batch_size=batch_size, device=device\n    )\n    \n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for i in range(len(test_image_inputs)):\n            outputs = model(test_image_inputs[i], test_text_inputs[i])\n            _, predicted = outputs.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predictions]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T11:31:48.195958Z","iopub.execute_input":"2025-01-10T11:31:48.196328Z","iopub.status.idle":"2025-01-10T11:31:50.563750Z","shell.execute_reply.started":"2025-01-10T11:31:48.196298Z","shell.execute_reply":"2025-01-10T11:31:50.562484Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dandelin/vilt-b32-mlm were not used when initializing ViltModel: ['mlm_score.transform.dense.bias', 'mlm_score.decoder.weight', 'mlm_score.transform.dense.weight', 'mlm_score.transform.LayerNorm.bias', 'mlm_score.bias', 'mlm_score.transform.LayerNorm.weight']\n- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3712848228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/3712848228.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mval_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3712848228.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_image_paths, train_texts, train_labels, val_image_paths, val_texts, val_labels, device, epochs, batch_size, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 device='cuda', epochs=30, batch_size=8, gradient_accumulation_steps=4):\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVILTSentimentModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 14.74 GiB total capacity; 14.11 GiB already allocated; 20.12 MiB free; 14.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 14.74 GiB total capacity; 14.11 GiB already allocated; 20.12 MiB free; 14.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"# Multimodal BERT (MBMT) v-1\nDefault MMBT implemenation\n\nEpoch: 30\\\nAccuracy: 74.92%\\\nVal. accuracy: 74.10%\\\n\nBatch size: 32\\\nDropout: 0.4\\\nWeight decay: 0.02","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom torchvision import transforms, models\nfrom torch import nn\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom sklearn.model_selection import train_test_split\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, hidden_size=512):\n        super().__init__()\n        resnet = models.resnet18(pretrained=True)\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.fc = nn.Linear(512, hidden_size)\n        \n    def forward(self, x):\n        x = self.resnet(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nclass MMBTSentimentModel(nn.Module):\n    def __init__(self, num_classes=3, hidden_size=512):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        \n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.image_encoder = ImageEncoder(hidden_size)\n        \n        for param in self.bert.parameters():\n            param.requires_grad = False\n            \n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size + self.bert.config.hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, num_classes)\n        )\n        \n        self.image_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                              std=[0.229, 0.224, 0.225])\n        ])\n    \n    def process_text(self, texts, device):\n        inputs = self.tokenizer(texts, padding=True, truncation=True, \n                              max_length=128, return_tensors=\"pt\")\n        return {k: v.to(device) for k, v in inputs.items()}\n    \n    def process_image(self, image_paths, device):\n        images = []\n        for path in image_paths:\n            try:\n                image = Image.open(path).convert('RGB')\n                image = self.image_transform(image)\n                images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                dummy = torch.zeros(3, 224, 224)\n                images.append(dummy)\n        return torch.stack(images).to(device)\n    \n    def forward(self, image_inputs, text_inputs):\n        text_outputs = self.bert(**text_inputs)\n        text_embeddings = text_outputs.pooler_output\n        \n        image_embeddings = self.image_encoder(image_inputs)\n        combined = torch.cat([text_embeddings, image_embeddings], dim=1)\n        \n        return self.classifier(combined)\n\ndef process_data_batch(image_paths, texts, model, batch_size=32, device='cuda'):\n    all_image_inputs = []\n    all_text_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = image_paths[i:i + batch_size]\n        batch_texts = texts[i:i + batch_size]\n        \n        images = model.process_image(batch_images, device)\n        text_inputs = model.process_text(batch_texts, device)\n        \n        all_image_inputs.append(images)\n        all_text_inputs.append(text_inputs)\n    \n    return all_image_inputs, all_text_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, \n                device='cuda', epochs=30, batch_size=32,\n                gradient_accumulation_steps=4):\n    \n    model = MMBTSentimentModel().to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.2)\n    criterion = nn.CrossEntropyLoss()\n    \n    print(\"Processing training data...\")\n    train_image_inputs, train_text_inputs = process_data_batch(\n        train_image_paths, train_texts, model, batch_size=batch_size, device=device\n    )\n    \n    print(\"Processing validation data...\")\n    val_image_inputs, val_text_inputs = process_data_batch(\n        val_image_paths, val_texts, model, batch_size=batch_size, device=device\n    )\n    \n    train_labels = torch.tensor(train_labels, device=device)\n    val_labels = torch.tensor(val_labels, device=device)\n    \n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for i in range(len(train_image_inputs)):\n            outputs = model(train_image_inputs[i], train_text_inputs[i])\n            batch_labels = train_labels[i * batch_size:(i + 1) * batch_size]\n            \n            loss = criterion(outputs, batch_labels) / gradient_accumulation_steps\n            loss.backward()\n            \n            if (i + 1) % gradient_accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            total_loss += loss.item() * gradient_accumulation_steps\n            _, predicted = outputs.max(1)\n            total += batch_labels.size(0)\n            correct += predicted.eq(batch_labels).sum().item()\n        \n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for i in range(len(val_image_inputs)):\n                outputs = model(val_image_inputs[i], val_text_inputs[i])\n                batch_labels = val_labels[i * batch_size:(i + 1) * batch_size]\n                \n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n        \n        val_acc = 100. * val_correct / val_total\n        print(f'Epoch {epoch + 1}: Train Acc: {100.*correct/total:.2f}% Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return model\n\ndef load_data(train_path, test_path):\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(folder, image_names):\n    return [os.path.join(folder, name) for name in image_names]\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    batch_size = 8\n    \n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    model = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs,\n        device=device,\n        batch_size=batch_size\n    )\n    \n    print(\"Processing test data...\")\n    test_image_inputs, test_text_inputs = process_data_batch(\n        test_image_paths, test_df['Captions'].tolist(),\n        model, batch_size=batch_size, device=device\n    )\n    \n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for i in range(len(test_image_inputs)):\n            outputs = model(test_image_inputs[i], test_text_inputs[i])\n            _, predicted = outputs.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predictions]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T11:52:14.120121Z","iopub.execute_input":"2025-01-10T11:52:14.120542Z","iopub.status.idle":"2025-01-10T12:00:12.075317Z","shell.execute_reply.started":"2025-01-10T11:52:14.120496Z","shell.execute_reply":"2025-01-10T12:00:12.073997Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\nProcessing validation data...\nEpoch 1: Train Acc: 69.73% Val Acc: 76.38%\nEpoch 2: Train Acc: 82.05% Val Acc: 71.62%\nEpoch 3: Train Acc: 93.37% Val Acc: 70.86%\nEpoch 4: Train Acc: 96.80% Val Acc: 69.71%\nEpoch 5: Train Acc: 97.31% Val Acc: 70.86%\nEpoch 6: Train Acc: 98.18% Val Acc: 73.71%\nEpoch 7: Train Acc: 98.79% Val Acc: 74.29%\nEpoch 8: Train Acc: 99.46% Val Acc: 74.86%\nEpoch 9: Train Acc: 99.46% Val Acc: 73.33%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1221044776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/1221044776.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mval_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/1221044776.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_image_paths, train_texts, train_labels, val_image_paths, val_texts, val_labels, device, epochs, batch_size, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"# VisualBERT","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, VisualBertModel, VisualBertConfig\nfrom torchvision import transforms, models\nfrom torch import nn\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom sklearn.model_selection import train_test_split\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, output_size=2048):\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        \n    def forward(self, x):\n        x = self.resnet(x)\n        return x.view(x.size(0), -1)\n\nclass VisualBERTSentimentModel(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        self.num_classes = num_classes\n        \n        config = VisualBertConfig.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n        self.visual_bert = VisualBertModel(config)\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.image_encoder = ImageEncoder()\n        \n        for param in self.visual_bert.parameters():\n            param.requires_grad = False\n            \n        self.classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, num_classes)\n        )\n        \n        self.image_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                              std=[0.229, 0.224, 0.225])\n        ])\n    \n    def process_text(self, texts, device):\n        inputs = self.tokenizer(texts, padding=True, truncation=True, \n                              max_length=128, return_tensors=\"pt\")\n        return {k: v.to(device) for k, v in inputs.items()}\n    \n    def process_image(self, image_paths, device):\n        images = []\n        for path in image_paths:\n            try:\n                image = Image.open(path).convert('RGB')\n                image = self.image_transform(image)\n                images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {path}: {str(e)}\")\n                dummy = torch.zeros(3, 224, 224)\n                images.append(dummy)\n        return torch.stack(images).to(device)\n    \n    def forward(self, image_inputs, text_inputs):\n        batch_size = image_inputs.size(0)\n        visual_embeds = self.image_encoder(image_inputs)\n            \n        # Reshape visual embeds to (batch_size, num_visual_features, visual_embedding_dim)\n        visual_embeds = visual_embeds.view(batch_size, 1, -1)\n            \n        # Create correct dimension masks\n        visual_attention_mask = torch.ones(batch_size, 1, dtype=torch.float).to(image_inputs.device)\n        visual_token_type_ids = torch.ones(batch_size, 1, dtype=torch.long).to(image_inputs.device)\n            \n        outputs = self.visual_bert(\n            input_ids=text_inputs['input_ids'],\n            attention_mask=text_inputs['attention_mask'],\n            visual_embeds=visual_embeds,\n            visual_token_type_ids=visual_token_type_ids,\n            visual_attention_mask=visual_attention_mask,\n            return_dict=True\n        )\n            \n        pooled_output = outputs.pooler_output\n        return self.classifier(pooled_output)\n\ndef process_data_batch(image_paths, texts, model, batch_size=8, device='cuda'):\n    all_image_inputs = []\n    all_text_inputs = []\n    \n    for i in range(0, len(image_paths), batch_size):\n        batch_images = image_paths[i:i + batch_size]\n        batch_texts = texts[i:i + batch_size]\n        \n        images = model.process_image(batch_images, device)\n        text_inputs = model.process_text(batch_texts, device)\n        \n        all_image_inputs.append(images)\n        all_text_inputs.append(text_inputs)\n    \n    return all_image_inputs, all_text_inputs\n\ndef train_model(train_image_paths, train_texts, train_labels, \n                val_image_paths, val_texts, val_labels, \n                device='cuda', epochs=30, batch_size=8,\n                gradient_accumulation_steps=4):\n    \n    model = VisualBERTSentimentModel().to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n    criterion = nn.CrossEntropyLoss()\n    \n    print(\"Processing training data...\")\n    train_image_inputs, train_text_inputs = process_data_batch(\n        train_image_paths, train_texts, model, batch_size=batch_size, device=device\n    )\n    \n    print(\"Processing validation data...\")\n    val_image_inputs, val_text_inputs = process_data_batch(\n        val_image_paths, val_texts, model, batch_size=batch_size, device=device\n    )\n    \n    train_labels = torch.tensor(train_labels, device=device)\n    val_labels = torch.tensor(val_labels, device=device)\n    \n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for i in range(len(train_image_inputs)):\n            outputs = model(train_image_inputs[i], train_text_inputs[i])\n            batch_labels = train_labels[i * batch_size:(i + 1) * batch_size]\n            \n            loss = criterion(outputs, batch_labels) / gradient_accumulation_steps\n            loss.backward()\n            \n            if (i + 1) % gradient_accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            total_loss += loss.item() * gradient_accumulation_steps\n            _, predicted = outputs.max(1)\n            total += batch_labels.size(0)\n            correct += predicted.eq(batch_labels).sum().item()\n        \n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for i in range(len(val_image_inputs)):\n                outputs = model(val_image_inputs[i], val_text_inputs[i])\n                batch_labels = val_labels[i * batch_size:(i + 1) * batch_size]\n                \n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n        \n        val_acc = 100. * val_correct / val_total\n        print(f'Epoch {epoch + 1}: Train Acc: {100.*correct/total:.2f}% Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return model\n\ndef load_data(train_path, test_path):\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(folder, image_names):\n    return [os.path.join(folder, name) for name in image_names]\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    batch_size = 8\n    \n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    train_paths, val_paths, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_image_paths, train_df['Captions'].tolist(),\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    model = train_model(\n        train_paths, train_texts, train_labs,\n        val_paths, val_texts, val_labs,\n        device=device,\n        batch_size=batch_size\n    )\n    \n    print(\"Processing test data...\")\n    test_image_inputs, test_text_inputs = process_data_batch(\n        test_image_paths, test_df['Captions'].tolist(),\n        model, batch_size=batch_size, device=device\n    )\n    \n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for i in range(len(test_image_inputs)):\n            outputs = model(test_image_inputs[i], test_text_inputs[i])\n            _, predicted = outputs.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predictions]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:08:37.947388Z","iopub.execute_input":"2025-01-10T12:08:37.948000Z","iopub.status.idle":"2025-01-10T12:10:24.617408Z","shell.execute_reply.started":"2025-01-10T12:08:37.947968Z","shell.execute_reply":"2025-01-10T12:10:24.616083Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Processing training data...\nError loading image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/570519310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/570519310.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mval_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/570519310.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_image_paths, train_texts, train_labels, val_image_paths, val_texts, val_labels, device, epochs, batch_size, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     train_image_inputs, train_text_inputs = process_data_batch(\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/570519310.py\u001b[0m in \u001b[0;36mprocess_data_batch\u001b[0;34m(image_paths, texts, model, batch_size, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mbatch_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mtext_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/570519310.py\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(self, image_paths, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.74 GiB total capacity; 12.82 GiB already allocated; 16.12 MiB free; 13.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.74 GiB total capacity; 12.82 GiB already allocated; 16.12 MiB free; 13.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}],"execution_count":13}]}